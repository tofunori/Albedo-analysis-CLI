{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Performance Bar Chart Generator\n",
    "\n",
    "This notebook creates the exact **3√ó4 method performance comparison** showing Correlation, RMSE, Bias, and MAE metrics across glaciers and MODIS methods.\n",
    "\n",
    "## Features\n",
    "- **3√ó4 bar chart matrix**: 3 glaciers √ó 4 metrics\n",
    "- **Method-specific color coding** with best performance highlighting\n",
    "- **Gold star indicators** for best performing methods\n",
    "- **Comprehensive statistical analysis** (R, RMSE, MAE, Bias)\n",
    "- **Support for all 3 glaciers**: Athabasca, Haig, Coropuna\n",
    "\n",
    "## Quick Start\n",
    "1. Run all cells in sequence\n",
    "2. The final plot will be displayed and saved as `method_performance_bar_chart_matrix.png`\n",
    "3. Customize the `CONFIG` section below if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update paths as needed for your system\n",
    "CONFIG = {\n",
    "    'data_paths': {\n",
    "        'athabasca': {\n",
    "            'modis': \"D:/Documents/Projects/athabasca_analysis/data/csv/Athabasca_Terra_Aqua_MultiProduct_2014-01-01_to_2021-01-01.csv\",\n",
    "            'aws': \"D:/Documents/Projects/athabasca_analysis/data/csv/iceAWS_Atha_albedo_daily_20152020_filled_clean.csv\"\n",
    "        },\n",
    "        'haig': {\n",
    "            'modis': \"D:/Documents/Projects/Haig_analysis/data/csv/Haig_MODIS_Pixel_Analysis_MultiProduct_2002_to_2016_fraction.csv\",\n",
    "            'aws': \"D:/Documents/Projects/Haig_analysis/data/csv/HaigAWS_daily_2002_2015_gapfilled.csv\"\n",
    "        },\n",
    "        'coropuna': {\n",
    "            'modis': \"D:/Documents/Projects/Coropuna_glacier/data/csv/coropuna_glacier_2014-01-01_to_2025-01-01.csv\",\n",
    "            'aws': \"D:/Documents/Projects/Coropuna_glacier/data/csv/COROPUNA_simple.csv\"\n",
    "        }\n",
    "    },\n",
    "    'aws_stations': {\n",
    "        'athabasca': {'lat': 52.1949, 'lon': -117.2431, 'name': 'Athabasca AWS'},\n",
    "        'haig': {'lat': 50.7186, 'lon': -115.3433, 'name': 'Haig AWS'},\n",
    "        'coropuna': {'lat': -15.5181, 'lon': -72.6617, 'name': 'Coropuna AWS'}\n",
    "    },\n",
    "    'colors': {\n",
    "        'athabasca': '#1f77b4',  # Blue\n",
    "        'haig': '#ff7f0e',       # Orange  \n",
    "        'coropuna': '#2ca02c',   # Green\n",
    "        'MOD09GA': '#9467bd',    # Purple\n",
    "        'MYD09GA': '#9467bd',    # Purple (same as MOD09GA)\n",
    "        'MCD43A3': '#d62728',    # Red\n",
    "        'MOD10A1': '#8c564b',    # Brown\n",
    "        'MYD10A1': '#8c564b',    # Brown (same as MOD10A1)\n",
    "        'mcd43a3': '#d62728',    # Red\n",
    "        'mod09ga': '#9467bd',    # Purple\n",
    "        'myd09ga': '#9467bd',    # Purple\n",
    "        'mod10a1': '#8c564b',    # Brown\n",
    "        'myd10a1': '#8c564b'     # Brown\n",
    "    },\n",
    "    'methods': ['MOD09GA', 'MYD09GA', 'MCD43A3', 'MOD10A1', 'MYD10A1'],\n",
    "    'method_mapping': {\n",
    "        # Map different case variations to standard format\n",
    "        'mcd43a3': 'MCD43A3',\n",
    "        'MCD43A3': 'MCD43A3',\n",
    "        'mod09ga': 'MOD09GA', \n",
    "        'MOD09GA': 'MOD09GA',\n",
    "        'myd09ga': 'MYD09GA',  # Keep Terra/Aqua separate for this visualization\n",
    "        'MYD09GA': 'MYD09GA',\n",
    "        'mod10a1': 'MOD10A1',\n",
    "        'MOD10A1': 'MOD10A1',\n",
    "        'myd10a1': 'MYD10A1',  # Keep Terra/Aqua separate for this visualization\n",
    "        'MYD10A1': 'MYD10A1'\n",
    "    },\n",
    "    'outlier_threshold': 2.5,\n",
    "    'quality_filters': {\n",
    "        'min_glacier_fraction': 0.1,\n",
    "        'min_observations': 10\n",
    "    },\n",
    "    'visualization': {\n",
    "        'figsize': (16, 12),\n",
    "        'dpi': 300,\n",
    "        'style': 'seaborn-v0_8'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"üìÅ Data paths configured for {len(CONFIG['data_paths'])} glaciers\")\n",
    "print(f\"üé® Color scheme includes {len(CONFIG['colors'])} method/glacier colors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Handles loading and preprocessing of MODIS and AWS data for all glaciers.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_glacier_data(self, glacier_id: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load MODIS and AWS data for a specific glacier.\"\"\"\n",
    "        logger.info(f\"Loading data for {glacier_id} glacier...\")\n",
    "        \n",
    "        paths = self.config['data_paths'][glacier_id]\n",
    "        \n",
    "        # Load MODIS data\n",
    "        modis_data = self._load_modis_data(paths['modis'], glacier_id)\n",
    "        \n",
    "        # Load AWS data\n",
    "        aws_data = self._load_aws_data(paths['aws'], glacier_id)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(modis_data):,} MODIS and {len(aws_data):,} AWS records for {glacier_id}\")\n",
    "        \n",
    "        return modis_data, aws_data\n",
    "    \n",
    "    def _load_modis_data(self, file_path: str, glacier_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Load MODIS data with glacier-specific parsing.\"\"\"\n",
    "        if not Path(file_path).exists():\n",
    "            raise FileNotFoundError(f\"MODIS data file not found: {file_path}\")\n",
    "        \n",
    "        logger.info(f\"Loading MODIS data from: {file_path}\")\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert date to datetime\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        \n",
    "        # Glacier-specific processing\n",
    "        if glacier_id == 'coropuna':\n",
    "            # Coropuna has method column - already in long format\n",
    "            if 'method' in data.columns and 'albedo' in data.columns:\n",
    "                logger.info(\"Coropuna data is in long format\")\n",
    "                # Apply method mapping to standardize names\n",
    "                data['method'] = data['method'].map(self.config['method_mapping']).fillna(data['method'])\n",
    "                return data\n",
    "        \n",
    "        # For other glaciers, check if conversion to long format is needed\n",
    "        if 'method' not in data.columns:\n",
    "            logger.info(f\"Converting {glacier_id} data to long format\")\n",
    "            data = self._convert_to_long_format(data, glacier_id)\n",
    "        else:\n",
    "            # Data already has method column, just apply mapping\n",
    "            logger.info(f\"{glacier_id} data already in long format\")\n",
    "            data['method'] = data['method'].map(self.config['method_mapping']).fillna(data['method'])\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _convert_to_long_format(self, data: pd.DataFrame, glacier_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Convert wide format MODIS data to long format.\"\"\"\n",
    "        long_format_rows = []\n",
    "        \n",
    "        # Define method mappings based on available columns\n",
    "        method_columns = {}\n",
    "        for col in data.columns:\n",
    "            if 'MOD09GA' in col and 'albedo' in col:\n",
    "                method_columns['MOD09GA'] = col\n",
    "            elif 'MYD09GA' in col and 'albedo' in col:\n",
    "                method_columns['MYD09GA'] = col\n",
    "            elif 'MOD10A1' in col and 'albedo' in col:\n",
    "                method_columns['MOD10A1'] = col\n",
    "            elif 'MYD10A1' in col and 'albedo' in col:\n",
    "                method_columns['MYD10A1'] = col\n",
    "            elif 'MCD43A3' in col and 'albedo' in col:\n",
    "                method_columns['MCD43A3'] = col\n",
    "            elif col in ['MOD09GA', 'MYD09GA', 'MOD10A1', 'MYD10A1', 'MCD43A3']:\n",
    "                method_columns[col] = col\n",
    "        \n",
    "        for method, col_name in method_columns.items():\n",
    "            if col_name not in data.columns:\n",
    "                continue\n",
    "                \n",
    "            # Extract data for this method\n",
    "            method_data = data[data[col_name].notna()][['pixel_id', 'date', col_name]].copy()\n",
    "            \n",
    "            if len(method_data) > 0:\n",
    "                method_data['method'] = method\n",
    "                method_data['albedo'] = method_data[col_name]\n",
    "                method_data = method_data.drop(columns=[col_name])\n",
    "                \n",
    "                # Add spatial coordinates if available\n",
    "                for coord_col in ['longitude', 'latitude']:\n",
    "                    if coord_col in data.columns:\n",
    "                        method_data[coord_col] = data.loc[method_data.index, coord_col]\n",
    "                \n",
    "                # Add glacier fraction if available\n",
    "                glacier_frac_cols = [c for c in data.columns if 'glacier_fraction' in c.lower()]\n",
    "                if glacier_frac_cols:\n",
    "                    method_data['glacier_fraction'] = data.loc[method_data.index, glacier_frac_cols[0]]\n",
    "                \n",
    "                long_format_rows.append(method_data)\n",
    "        \n",
    "        if long_format_rows:\n",
    "            return pd.concat(long_format_rows, ignore_index=True)\n",
    "        else:\n",
    "            logger.error(f\"No valid method data found for {glacier_id}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _load_aws_data(self, file_path: str, glacier_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Load AWS data with glacier-specific parsing.\"\"\"\n",
    "        if not Path(file_path).exists():\n",
    "            raise FileNotFoundError(f\"AWS data file not found: {file_path}\")\n",
    "        \n",
    "        logger.info(f\"Loading AWS data from: {file_path}\")\n",
    "        \n",
    "        if glacier_id == 'haig':\n",
    "            # Haig has special format: semicolon separated, European decimal\n",
    "            aws_data = pd.read_csv(file_path, sep=';', skiprows=6, decimal=',')\n",
    "            aws_data.columns = aws_data.columns.str.strip()\n",
    "            \n",
    "            # Process Year and Day columns to create datetime\n",
    "            aws_data = aws_data.dropna(subset=['Year', 'Day'])\n",
    "            aws_data['Year'] = aws_data['Year'].astype(int)\n",
    "            aws_data['Day'] = aws_data['Day'].astype(int)\n",
    "            \n",
    "            # Convert Day of Year to datetime\n",
    "            aws_data['date'] = pd.to_datetime(\n",
    "                aws_data['Year'].astype(str) + '-01-01'\n",
    "            ) + pd.to_timedelta(aws_data['Day'] - 1, unit='D')\n",
    "            \n",
    "            # Find albedo column\n",
    "            albedo_cols = [col for col in aws_data.columns if 'albedo' in col.lower()]\n",
    "            if albedo_cols:\n",
    "                albedo_col = albedo_cols[0]\n",
    "                aws_data['Albedo'] = pd.to_numeric(aws_data[albedo_col], errors='coerce')\n",
    "            else:\n",
    "                raise ValueError(f\"No albedo column found in Haig AWS data\")\n",
    "                \n",
    "        elif glacier_id == 'coropuna':\n",
    "            # Coropuna format: Timestamp, Albedo\n",
    "            aws_data = pd.read_csv(file_path)\n",
    "            aws_data['date'] = pd.to_datetime(aws_data['Timestamp'])\n",
    "            \n",
    "        elif glacier_id == 'athabasca':\n",
    "            # Athabasca format: Time, Albedo\n",
    "            aws_data = pd.read_csv(file_path)\n",
    "            aws_data['date'] = pd.to_datetime(aws_data['Time'])\n",
    "        \n",
    "        # Clean and validate data\n",
    "        aws_data = aws_data[['date', 'Albedo']].copy()\n",
    "        aws_data = aws_data.dropna(subset=['Albedo'])\n",
    "        aws_data = aws_data[aws_data['Albedo'] > 0]  # Remove invalid albedo values\n",
    "        aws_data = aws_data.drop_duplicates().sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        return aws_data\n",
    "\n",
    "print(\"‚úÖ DataLoader class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pixel Selection and Data Processing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelSelector:\n",
    "    \"\"\"Implements intelligent pixel selection based on distance to AWS stations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        \n",
    "    def select_best_pixels(self, modis_data: pd.DataFrame, glacier_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Select best pixels for analysis based on AWS distance and glacier fraction.\"\"\"\n",
    "        logger.info(f\"Applying pixel selection for {glacier_id}...\")\n",
    "        \n",
    "        # Get AWS station coordinates\n",
    "        aws_station = self.config['aws_stations'][glacier_id]\n",
    "        aws_lat, aws_lon = aws_station['lat'], aws_station['lon']\n",
    "        \n",
    "        # Get available pixels with their quality metrics\n",
    "        pixel_summary = modis_data.groupby('pixel_id').agg({\n",
    "            'glacier_fraction': 'mean',\n",
    "            'albedo': 'count',\n",
    "            'latitude': 'first',\n",
    "            'longitude': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        pixel_summary.columns = ['pixel_id', 'avg_glacier_fraction', 'n_observations', 'latitude', 'longitude']\n",
    "        \n",
    "        # Apply quality filters\n",
    "        quality_filters = self.config['quality_filters']\n",
    "        quality_pixels = pixel_summary[\n",
    "            (pixel_summary['avg_glacier_fraction'] > quality_filters['min_glacier_fraction']) & \n",
    "            (pixel_summary['n_observations'] > quality_filters['min_observations'])\n",
    "        ].copy()\n",
    "        \n",
    "        if len(quality_pixels) == 0:\n",
    "            logger.warning(f\"No quality pixels found for {glacier_id}, using all data\")\n",
    "            return modis_data\n",
    "        \n",
    "        # Calculate distance to AWS station using Haversine formula\n",
    "        quality_pixels['distance_to_aws'] = self._haversine_distance(\n",
    "            quality_pixels['latitude'], quality_pixels['longitude'], aws_lat, aws_lon\n",
    "        )\n",
    "        \n",
    "        # For Athabasca (small dataset), use all pixels\n",
    "        if glacier_id == 'athabasca':\n",
    "            selected_pixel_ids = quality_pixels['pixel_id'].tolist()\n",
    "            logger.info(f\"Using all {len(selected_pixel_ids)} pixels for {glacier_id} (small dataset)\")\n",
    "        else:\n",
    "            # Sort by glacier fraction (descending) then distance (ascending)\n",
    "            quality_pixels = quality_pixels.sort_values([\n",
    "                'avg_glacier_fraction', 'distance_to_aws'\n",
    "            ], ascending=[False, True])\n",
    "            \n",
    "            # Select the best performing pixel\n",
    "            selected_pixels = quality_pixels.head(1)\n",
    "            selected_pixel_ids = selected_pixels['pixel_id'].tolist()\n",
    "            \n",
    "            logger.info(f\"Selected {len(selected_pixel_ids)} best pixel(s) for {glacier_id}\")\n",
    "            for _, pixel in selected_pixels.iterrows():\n",
    "                logger.info(f\"  Pixel {pixel['pixel_id']}: \"\n",
    "                           f\"glacier_fraction={pixel['avg_glacier_fraction']:.3f}, \"\n",
    "                           f\"distance={pixel['distance_to_aws']:.2f}km, \"\n",
    "                           f\"observations={pixel['n_observations']}\")\n",
    "        \n",
    "        # Filter MODIS data to selected pixels\n",
    "        filtered_data = modis_data[modis_data['pixel_id'].isin(selected_pixel_ids)].copy()\n",
    "        logger.info(f\"Filtered MODIS data from {len(modis_data)} to {len(filtered_data)} observations\")\n",
    "        \n",
    "        return filtered_data\n",
    "    \n",
    "    def _haversine_distance(self, lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate distance using Haversine formula.\"\"\"\n",
    "        R = 6371  # Earth's radius in km\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        return R * c\n",
    "\n",
    "print(\"‚úÖ PixelSelector class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Handles AWS-MODIS data merging and statistical processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        \n",
    "    def merge_and_process(self, modis_data: pd.DataFrame, aws_data: pd.DataFrame, \n",
    "                         glacier_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Merge AWS and MODIS data and calculate statistics for each method.\"\"\"\n",
    "        logger.info(f\"Merging and processing data for {glacier_id}...\")\n",
    "        \n",
    "        results = []\n",
    "        available_methods = modis_data['method'].unique()\n",
    "        \n",
    "        for method in available_methods:\n",
    "            # Filter MODIS data for this method\n",
    "            method_data = modis_data[modis_data['method'] == method].copy()\n",
    "            \n",
    "            if len(method_data) == 0:\n",
    "                logger.warning(f\"No {method} data found for {glacier_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Merge with AWS data on date\n",
    "            merged = method_data.merge(aws_data, on='date', how='inner')\n",
    "            \n",
    "            if len(merged) < 3:  # Need minimum data points\n",
    "                logger.warning(f\"Insufficient {method} data for {glacier_id}: {len(merged)} points\")\n",
    "                continue\n",
    "            \n",
    "            # Apply outlier filtering\n",
    "            aws_clean, modis_clean = self._apply_outlier_filtering(\n",
    "                merged['Albedo'].values, merged['albedo'].values\n",
    "            )\n",
    "            \n",
    "            if len(aws_clean) < 3:\n",
    "                logger.warning(f\"Insufficient {method} data after outlier filtering for {glacier_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = self._calculate_statistics(aws_clean, modis_clean)\n",
    "            \n",
    "            results.append({\n",
    "                'glacier_id': glacier_id,\n",
    "                'method': method,\n",
    "                'aws_values': aws_clean,\n",
    "                'modis_values': modis_clean,\n",
    "                **stats\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"Processed {method} for {glacier_id}: \"\n",
    "                       f\"{len(aws_clean)} samples, r={stats['r']:.3f}\")\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def _apply_outlier_filtering(self, aws_vals: np.ndarray, modis_vals: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Apply 2.5œÉ outlier filtering to AWS-MODIS pairs.\"\"\"\n",
    "        if len(aws_vals) < 3:\n",
    "            return aws_vals, modis_vals\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = modis_vals - aws_vals\n",
    "        mean_residual = np.mean(residuals)\n",
    "        std_residual = np.std(residuals)\n",
    "        \n",
    "        # Apply threshold\n",
    "        threshold = self.config['outlier_threshold'] * std_residual\n",
    "        mask = np.abs(residuals - mean_residual) <= threshold\n",
    "        \n",
    "        return aws_vals[mask], modis_vals[mask]\n",
    "    \n",
    "    def _calculate_statistics(self, aws_vals: np.ndarray, modis_vals: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Calculate comprehensive statistics between AWS and MODIS values.\"\"\"\n",
    "        if len(aws_vals) == 0:\n",
    "            return {\n",
    "                'r': np.nan, 'rmse': np.nan, 'mae': np.nan, 'bias': np.nan, \n",
    "                'n_samples': 0, 'p_value': 1.0\n",
    "            }\n",
    "        \n",
    "        # Basic statistics\n",
    "        correlation = np.corrcoef(aws_vals, modis_vals)[0, 1] if len(aws_vals) > 1 else np.nan\n",
    "        \n",
    "        # Error metrics\n",
    "        residuals = modis_vals - aws_vals\n",
    "        rmse = np.sqrt(np.mean(residuals**2))\n",
    "        mae = np.mean(np.abs(residuals))\n",
    "        bias = np.mean(residuals)\n",
    "        \n",
    "        # Statistical significance\n",
    "        if len(aws_vals) > 2 and not np.isnan(correlation):\n",
    "            t_stat = correlation * np.sqrt((len(aws_vals) - 2) / (1 - correlation**2))\n",
    "            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), len(aws_vals) - 2))\n",
    "        else:\n",
    "            p_value = 1.0\n",
    "        \n",
    "        return {\n",
    "            'r': correlation if not np.isnan(correlation) else 0.0,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'bias': bias,\n",
    "            'n_samples': len(aws_vals),\n",
    "            'p_value': p_value\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ DataProcessor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Method Performance Visualization Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MethodPerformanceVisualizer:\n",
    "    \"\"\"Creates the 3√ó4 method performance bar chart matrix.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        \n",
    "    def create_performance_matrix(self, processed_data: List[pd.DataFrame], \n",
    "                                 output_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"Create the 3√ó4 method performance bar chart matrix.\"\"\"\n",
    "        logger.info(\"Creating method performance bar chart matrix...\")\n",
    "        \n",
    "        # Set matplotlib style\n",
    "        try:\n",
    "            plt.style.use(self.config['visualization']['style'])\n",
    "        except:\n",
    "            logger.warning(\"Could not set plotting style, using default\")\n",
    "        \n",
    "        # Create 3x4 subplot layout (3 glaciers x 4 metrics)\n",
    "        fig, axes = plt.subplots(3, 4, figsize=self.config['visualization']['figsize'])\n",
    "        \n",
    "        # Enhanced title with pixel selection information\n",
    "        main_title = 'Method Performance by Glacier and Metric'\n",
    "        subtitle = \"Selected Best Pixels: 2/1/1 (Closest to AWS Stations)\"\n",
    "        fig.suptitle(f'{main_title}\\n{subtitle}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        metrics = ['r', 'rmse', 'bias', 'mae']\n",
    "        metric_titles = ['Correlation (r)', 'RMSE', 'Bias', 'MAE']\n",
    "        glaciers = ['athabasca', 'coropuna', 'haig']  # Match your image order\n",
    "        \n",
    "        # Create combined dataframe for easier processing\n",
    "        all_data = pd.concat(processed_data, ignore_index=True) if processed_data else pd.DataFrame()\n",
    "        \n",
    "        # Create plots for each glacier-metric combination\n",
    "        for glacier_idx, glacier_id in enumerate(glaciers):\n",
    "            glacier_data = all_data[all_data['glacier_id'] == glacier_id] if not all_data.empty else pd.DataFrame()\n",
    "            \n",
    "            for metric_idx, (metric, metric_title) in enumerate(zip(metrics, metric_titles)):\n",
    "                ax = axes[glacier_idx, metric_idx]\n",
    "                \n",
    "                if not glacier_data.empty:\n",
    "                    # Prepare data for this glacier-metric combination\n",
    "                    method_values = []\n",
    "                    method_labels = []\n",
    "                    colors = []\n",
    "                    \n",
    "                    # Get available methods for this glacier, sorted\n",
    "                    available_methods = sorted(glacier_data['method'].unique())\n",
    "                    \n",
    "                    for method in available_methods:\n",
    "                        method_data = glacier_data[glacier_data['method'] == method]\n",
    "                        if not method_data.empty:\n",
    "                            method_values.append(method_data[metric].iloc[0])\n",
    "                            method_labels.append(method)\n",
    "                            colors.append(self.config['colors'].get(method, 'gray'))\n",
    "                    \n",
    "                    if method_values:  # If we have data to plot\n",
    "                        # Create bar chart\n",
    "                        bars = ax.bar(range(len(method_labels)), method_values, \n",
    "                                     color=colors, alpha=0.7, edgecolor='black', linewidth=1)\n",
    "                        \n",
    "                        # Add value labels on bars\n",
    "                        for i, (bar, value) in enumerate(zip(bars, method_values)):\n",
    "                            height = bar.get_height()\n",
    "                            # Position text above or below bar depending on value\n",
    "                            if metric == 'bias' and value < 0:\n",
    "                                va = 'top'\n",
    "                                y_pos = height - abs(height)*0.02\n",
    "                            else:\n",
    "                                va = 'bottom'\n",
    "                                y_pos = height + abs(height)*0.02\n",
    "                            \n",
    "                            ax.text(bar.get_x() + bar.get_width()/2., y_pos,\n",
    "                                   f'{value:.3f}', ha='center', va=va, \n",
    "                                   fontsize=9, fontweight='bold')\n",
    "                        \n",
    "                        # Highlight best performing method\n",
    "                        if len(method_values) > 1:\n",
    "                            if metric == 'r':  # Higher is better\n",
    "                                best_idx = method_values.index(max(method_values))\n",
    "                            elif metric in ['rmse', 'mae']:  # Lower is better\n",
    "                                best_idx = method_values.index(min(method_values))\n",
    "                            elif metric == 'bias':  # Closest to zero is better\n",
    "                                best_idx = method_values.index(min(method_values, key=abs))\n",
    "                            \n",
    "                            # Add gold highlighting to best method\n",
    "                            bars[best_idx].set_edgecolor('gold')\n",
    "                            bars[best_idx].set_linewidth(3)\n",
    "                            \n",
    "                            # Add gold star\n",
    "                            best_value = method_values[best_idx]\n",
    "                            star_y = best_value + abs(best_value)*0.08 if best_value >= 0 else best_value - abs(best_value)*0.08\n",
    "                            ax.text(best_idx, star_y, '‚òÖ', ha='center', va='center', \n",
    "                                   fontsize=12, color='gold', fontweight='bold')\n",
    "                        \n",
    "                        # Customize subplot\n",
    "                        ax.set_xticks(range(len(method_labels)))\n",
    "                        ax.set_xticklabels(method_labels, rotation=45, ha='right')\n",
    "                        ax.grid(True, alpha=0.3, axis='y')\n",
    "                        \n",
    "                        # Set y-axis limits for better comparison across glaciers\n",
    "                        if metric == 'r':\n",
    "                            ax.set_ylim(0, 1)\n",
    "                        elif metric in ['rmse', 'mae']:\n",
    "                            # Set common scale for error metrics\n",
    "                            all_values = all_data[metric].values\n",
    "                            ax.set_ylim(0, max(all_values) * 1.15)\n",
    "                        elif metric == 'bias':\n",
    "                            # Center bias around zero\n",
    "                            all_values = all_data[metric].values\n",
    "                            max_abs = max(abs(all_values)) if len(all_values) > 0 else 0.1\n",
    "                            ax.set_ylim(-max_abs * 1.2, max_abs * 1.2)\n",
    "                            ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "                    else:\n",
    "                        # No data available\n",
    "                        ax.text(0.5, 0.5, 'No data available', transform=ax.transAxes,\n",
    "                               ha='center', va='center', fontsize=10, style='italic')\n",
    "                else:\n",
    "                    # No data for this glacier\n",
    "                    ax.text(0.5, 0.5, 'No data available', transform=ax.transAxes,\n",
    "                           ha='center', va='center', fontsize=10, style='italic')\n",
    "                \n",
    "                # Add glacier name to leftmost plots\n",
    "                if metric_idx == 0:\n",
    "                    ax.set_ylabel(f'{glacier_id.title()}\\n{metric_title}', fontweight='bold')\n",
    "                else:\n",
    "                    ax.set_ylabel(metric_title)\n",
    "                \n",
    "                # Add metric title to top row\n",
    "                if glacier_idx == 0:\n",
    "                    ax.set_title(metric_title, fontweight='bold')\n",
    "        \n",
    "        # Add legend for method colors (show all possible methods)\n",
    "        legend_methods = ['MOD09GA', 'MYD09GA', 'MCD43A3', 'MOD10A1', 'MYD10A1']\n",
    "        legend_elements = [plt.Rectangle((0,0),1,1, facecolor=self.config['colors'].get(method, 'gray'), \n",
    "                                       edgecolor='black', alpha=0.7, label=method) \n",
    "                          for method in legend_methods]\n",
    "        legend_elements.append(plt.Rectangle((0,0),1,1, facecolor='none', \n",
    "                                           edgecolor='gold', linewidth=3, \n",
    "                                           label='Best Performance'))\n",
    "        \n",
    "        fig.legend(legend_elements, [elem.get_label() for elem in legend_elements], \n",
    "                  loc='center', bbox_to_anchor=(0.5, 0.02), ncol=6, fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.08)  # Make room for legend\n",
    "        \n",
    "        # Save figure if path provided\n",
    "        if output_path:\n",
    "            fig.savefig(output_path, dpi=self.config['visualization']['dpi'], \n",
    "                       bbox_inches='tight', facecolor='white')\n",
    "            logger.info(f\"Method performance matrix saved to: {output_path}\")\n",
    "        \n",
    "        return fig\n",
    "\n",
    "print(\"‚úÖ MethodPerformanceVisualizer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Processing Pipeline\n",
    "\n",
    "Now let's run the complete pipeline to load data, process it, and create the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "data_loader = DataLoader(CONFIG)\n",
    "pixel_selector = PixelSelector(CONFIG)\n",
    "data_processor = DataProcessor(CONFIG)\n",
    "visualizer = MethodPerformanceVisualizer(CONFIG)\n",
    "\n",
    "print(\"‚úÖ All components initialized\")\n",
    "print(\"üìä Ready to process glacier data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each glacier\n",
    "all_processed_data = []\n",
    "\n",
    "for glacier_id in ['athabasca', 'haig', 'coropuna']:\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing {glacier_id.upper()} Glacier\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Load data\n",
    "        modis_data, aws_data = data_loader.load_glacier_data(glacier_id)\n",
    "        \n",
    "        # Display data summary\n",
    "        print(f\"üìä Data Summary for {glacier_id}:\")\n",
    "        print(f\"   MODIS observations: {len(modis_data):,}\")\n",
    "        print(f\"   AWS observations: {len(aws_data):,}\")\n",
    "        \n",
    "        if 'method' in modis_data.columns:\n",
    "            available_methods = modis_data['method'].unique()\n",
    "            print(f\"   Available methods: {list(available_methods)}\")\n",
    "        \n",
    "        # Apply pixel selection\n",
    "        selected_modis = pixel_selector.select_best_pixels(modis_data, glacier_id)\n",
    "        \n",
    "        # Process and merge data\n",
    "        processed = data_processor.merge_and_process(selected_modis, aws_data, glacier_id)\n",
    "        \n",
    "        if not processed.empty:\n",
    "            all_processed_data.append(processed)\n",
    "            print(f\"‚úÖ Successfully processed {glacier_id}: {len(processed)} methods with valid data\")\n",
    "            \n",
    "            # Show summary statistics\n",
    "            print(f\"üìà Statistics summary:\")\n",
    "            for _, row in processed.iterrows():\n",
    "                print(f\"   {row['method']}: r={row['r']:.3f}, RMSE={row['rmse']:.3f}, n={row['n_samples']}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  No processed data for {glacier_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {glacier_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüìä Processing complete! {len(all_processed_data)} glaciers ready for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create the Method Performance Visualization\n",
    "\n",
    "Now let's generate the final 3√ó4 bar chart matrix showing method performance across glaciers and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "if all_processed_data:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Creating Method Performance Bar Chart Matrix\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate output filename\n",
    "    output_path = \"method_performance_bar_chart_matrix.png\"\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = visualizer.create_performance_matrix(all_processed_data, output_path)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ SUCCESS: Method performance matrix generated and saved to {output_path}\")\n",
    "    print(f\"üìä Total glaciers processed: {len(all_processed_data)}\")\n",
    "    \n",
    "    # Show summary of all processed data\n",
    "    combined_data = pd.concat(all_processed_data, ignore_index=True)\n",
    "    print(f\"\\nüìà Overall Summary:\")\n",
    "    print(f\"   Total glacier-method combinations: {len(combined_data)}\")\n",
    "    print(f\"   Methods represented: {sorted(combined_data['method'].unique())}\")\n",
    "    print(f\"   Glaciers represented: {sorted(combined_data['glacier_id'].unique())}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data could be processed for any glacier\")\n",
    "    print(\"Please check your data file paths in the CONFIG section above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis Summary\n",
    "\n",
    "Let's display a summary table of all the statistics calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics table\n",
    "if all_processed_data:\n",
    "    combined_data = pd.concat(all_processed_data, ignore_index=True)\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_table = combined_data[['glacier_id', 'method', 'r', 'rmse', 'bias', 'mae', 'n_samples']].copy()\n",
    "    summary_table = summary_table.round(3)\n",
    "    \n",
    "    print(\"\\nüìä COMPLETE STATISTICS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary_table.to_string(index=False))\n",
    "    \n",
    "    # Best performers by metric\n",
    "    print(\"\\nüèÜ BEST PERFORMERS BY METRIC\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Best correlation\n",
    "    best_r = summary_table.loc[summary_table['r'].idxmax()]\n",
    "    print(f\"Highest Correlation: {best_r['method']} on {best_r['glacier_id']} (r = {best_r['r']:.3f})\")\n",
    "    \n",
    "    # Lowest RMSE\n",
    "    best_rmse = summary_table.loc[summary_table['rmse'].idxmin()]\n",
    "    print(f\"Lowest RMSE: {best_rmse['method']} on {best_rmse['glacier_id']} (RMSE = {best_rmse['rmse']:.3f})\")\n",
    "    \n",
    "    # Lowest absolute bias\n",
    "    summary_table['abs_bias'] = abs(summary_table['bias'])\n",
    "    best_bias = summary_table.loc[summary_table['abs_bias'].idxmin()]\n",
    "    print(f\"Lowest Bias: {best_bias['method']} on {best_bias['glacier_id']} (bias = {best_bias['bias']:.3f})\")\n",
    "    \n",
    "    # Lowest MAE\n",
    "    best_mae = summary_table.loc[summary_table['mae'].idxmin()]\n",
    "    print(f\"Lowest MAE: {best_mae['method']} on {best_mae['glacier_id']} (MAE = {best_mae['mae']:.3f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Customization Options\n",
    "\n",
    "You can customize the analysis by modifying the CONFIG dictionary above:\n",
    "\n",
    "### üìÅ **Data Paths**\n",
    "Update `CONFIG['data_paths']` to point to your CSV files\n",
    "\n",
    "### üé® **Colors**\n",
    "Modify `CONFIG['colors']` to change method and glacier colors\n",
    "\n",
    "### üìä **Quality Filters**\n",
    "Adjust `CONFIG['quality_filters']` to change pixel selection criteria:\n",
    "- `min_glacier_fraction`: Minimum glacier coverage (default: 0.1)\n",
    "- `min_observations`: Minimum number of observations (default: 10)\n",
    "\n",
    "### üîß **Processing Parameters**\n",
    "- `outlier_threshold`: Sigma threshold for outlier removal (default: 2.5)\n",
    "- `visualization.figsize`: Plot dimensions (default: (16, 12))\n",
    "- `visualization.dpi`: Output resolution (default: 300)\n",
    "\n",
    "### üîÑ **Re-run Analysis**\n",
    "After making changes, re-run the cells starting from \"Data Processing Pipeline\" to see the effects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}