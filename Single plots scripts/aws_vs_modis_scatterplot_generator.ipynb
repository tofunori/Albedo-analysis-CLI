{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS vs MODIS Albedo Scatterplot Matrix Generator\n",
    "\n",
    "This notebook recreates the exact 3×3 AWS vs MODIS albedo correlation plots from the multi-glacier comparative analysis. It includes the complete data pipeline from raw CSV files to publication-ready visualizations.\n",
    "\n",
    "## Features\n",
    "- **Complete Data Pipeline**: Loads data from all 3 glaciers with proper format handling\n",
    "- **Intelligent Pixel Selection**: Uses distance to AWS stations + glacier fraction weighting  \n",
    "- **Outlier Filtering**: Applies 2.5σ threshold for robust statistics\n",
    "- **Comprehensive Statistics**: Calculates R, R², RMSE, MAE, Bias for each method\n",
    "- **Publication-Ready Plots**: Exact 3×3 matrix with proper colors and styling\n",
    "- **Interactive Analysis**: Easy to modify parameters and explore results\n",
    "\n",
    "## Glaciers Analyzed\n",
    "- **Athabasca** (Canadian Rockies): Blue color, all 2 pixels used\n",
    "- **Haig** (Canadian Rockies): Orange color, best pixel selected\n",
    "- **Coropuna** (Peruvian Andes): Green color, best pixel selected\n",
    "\n",
    "## MODIS Methods\n",
    "- **MCD43A3**: Daily BRDF-adjusted reflectance\n",
    "- **MOD09GA**: Daily surface reflectance\n",
    "- **MOD10A1**: Daily snow cover\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging for notebook\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Modify these paths to match your system\n",
    "CONFIG = {\n",
    "    'data_paths': {\n",
    "        'athabasca': {\n",
    "            'modis': \"D:/Documents/Projects/athabasca_analysis/data/csv/Athabasca_Terra_Aqua_MultiProduct_2014-01-01_to_2021-01-01.csv\",\n",
    "            'aws': \"D:/Documents/Projects/athabasca_analysis/data/csv/iceAWS_Atha_albedo_daily_20152020_filled_clean.csv\"\n",
    "        },\n",
    "        'haig': {\n",
    "            'modis': \"D:/Documents/Projects/Haig_analysis/data/csv/Haig_MODIS_Pixel_Analysis_MultiProduct_2002_to_2016_fraction.csv\",\n",
    "            'aws': \"D:/Documents/Projects/Haig_analysis/data/csv/HaigAWS_daily_2002_2015_gapfilled.csv\"\n",
    "        },\n",
    "        'coropuna': {\n",
    "            'modis': \"D:/Documents/Projects/Coropuna_glacier/data/csv/coropuna_glacier_2014-01-01_to_2025-01-01.csv\",\n",
    "            'aws': \"D:/Documents/Projects/Coropuna_glacier/data/csv/COROPUNA_simple.csv\"\n",
    "        }\n",
    "    },\n",
    "    'aws_stations': {\n",
    "        'athabasca': {'lat': 52.1949, 'lon': -117.2431, 'name': 'Athabasca AWS'},\n",
    "        'haig': {'lat': 50.7186, 'lon': -115.3433, 'name': 'Haig AWS'},\n",
    "        'coropuna': {'lat': -15.5181, 'lon': -72.6617, 'name': 'Coropuna AWS'}\n",
    "    },\n",
    "    'colors': {\n",
    "        'athabasca': '#1f77b4',  # Blue\n",
    "        'haig': '#ff7f0e',       # Orange  \n",
    "        'coropuna': '#2ca02c'    # Green\n",
    "    },\n",
    "    'methods': ['MCD43A3', 'MOD09GA', 'MOD10A1'],\n",
    "    'outlier_threshold': 2.5,\n",
    "    'quality_filters': {\n",
    "        'min_glacier_fraction': 0.1,\n",
    "        'min_observations': 10\n",
    "    },\n",
    "    'visualization': {\n",
    "        'figsize': (15, 12),\n",
    "        'dpi': 300,\n",
    "        'style': 'seaborn-v0_8'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"Glaciers: {list(CONFIG['data_paths'].keys())}\")\n",
    "print(f\"Methods: {CONFIG['methods']}\")\n",
    "print(f\"Outlier threshold: {CONFIG['outlier_threshold']}σ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Handles loading and preprocessing of MODIS and AWS data for all glaciers.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_glacier_data(self, glacier_id: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load MODIS and AWS data for a specific glacier.\"\"\"\n",
    "        logger.info(f\"Loading data for {glacier_id} glacier...\")\n",
    "        \n",
    "        paths = self.config['data_paths'][glacier_id]\n",
    "        \n",
    "        # Load MODIS data\n",
    "        modis_data = self._load_modis_data(paths['modis'], glacier_id)\n",
    "        \n",
    "        # Load AWS data\n",
    "        aws_data = self._load_aws_data(paths['aws'], glacier_id)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(modis_data):,} MODIS and {len(aws_data):,} AWS records for {glacier_id}\")\n",
    "        \n",
    "        return modis_data, aws_data\n",
    "    \n",
    "    def _load_modis_data(self, file_path: str, glacier_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Load MODIS data with glacier-specific parsing.\"\"\"\n",
    "        if not Path(file_path).exists():\n",
    "            raise FileNotFoundError(f\"MODIS data file not found: {file_path}\")\n",
    "        \n",
    "        logger.info(f\"Loading MODIS data from: {file_path}\")\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert date to datetime\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        \n",
    "        # Glacier-specific processing\n",
    "        if glacier_id == 'coropuna':\n",
    "            # Coropuna has method column - already in long format\n",
    "            if 'method' in data.columns and 'albedo' in data.columns:\n",
    "                logger.info(\"Coropuna data is in long format\")\n",
    "                return data\n",
    "        \n",
    "        # For other glaciers, check if conversion to long format is needed\n",
    "        if 'method' not in data.columns:\n",
    "            logger.info(f\"Converting {glacier_id} data to long format\")\n",
    "            data = self._convert_to_long_format(data, glacier_id)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _convert_to_long_format(self, data: pd.DataFrame, glacier_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Convert wide format MODIS data to long format.\"\"\"\n",
    "        long_format_rows = []\n",
    "        \n",
    "        # Define method mappings based on available columns\n",
    "        method_columns = {}\n",
    "        for col in data.columns:\n",
    "            if 'MOD09GA' in col and 'albedo' in col:\n",
    "                method_columns['MOD09GA'] = col\n",
    "            elif 'MOD10A1' in col and 'albedo' in col:\n",
    "                method_columns['MOD10A1'] = col\n",
    "            elif 'MCD43A3' in col and 'albedo' in col:\n",
    "                method_columns['MCD43A3'] = col\n",
    "            elif col in ['MOD09GA', 'MOD10A1', 'MCD43A3']:\n",
    "                method_columns[col] = col\n",
    "        \n",
    "        for method, col_name in method_columns.items():\n",
    "            if col_name not in data.columns:\n",
    "                continue\n",
    "                \n",
    "            # Extract data for this method\n",
    "            method_data = data[data[col_name].notna()][['pixel_id', 'date', col_name]].copy()\n",
    "            \n",
    "            if len(method_data) > 0:\n",
    "                method_data['method'] = method\n",
    "                method_data['albedo'] = method_data[col_name]\n",
    "                method_data = method_data.drop(columns=[col_name])\n",
    "                \n",
    "                # Add spatial coordinates if available\n",
    "                for coord_col in ['longitude', 'latitude']:\n",
    "                    if coord_col in data.columns:\n",
    "                        method_data[coord_col] = data.loc[method_data.index, coord_col]\n",
    "                \n",
    "                # Add glacier fraction if available\n",
    "                glacier_frac_cols = [c for c in data.columns if 'glacier_fraction' in c.lower()]\n",
    "                if glacier_frac_cols:\n",
    "                    method_data['glacier_fraction'] = data.loc[method_data.index, glacier_frac_cols[0]]\n",
    "                \n",
    "                long_format_rows.append(method_data)\n",
    "        \n",
    "        if long_format_rows:\n",
    "            return pd.concat(long_format_rows, ignore_index=True)\n",
    "        else:\n",
    "            logger.error(f\"No valid method data found for {glacier_id}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _load_aws_data(self, file_path: str, glacier_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Load AWS data with glacier-specific parsing.\"\"\"\n",
    "        if not Path(file_path).exists():\n",
    "            raise FileNotFoundError(f\"AWS data file not found: {file_path}\")\n",
    "        \n",
    "        logger.info(f\"Loading AWS data from: {file_path}\")\n",
    "        \n",
    "        if glacier_id == 'haig':\n",
    "            # Haig has special format: semicolon separated, European decimal\n",
    "            aws_data = pd.read_csv(file_path, sep=';', skiprows=6, decimal=',')\n",
    "            aws_data.columns = aws_data.columns.str.strip()\n",
    "            \n",
    "            # Process Year and Day columns to create datetime\n",
    "            aws_data = aws_data.dropna(subset=['Year', 'Day'])\n",
    "            aws_data['Year'] = aws_data['Year'].astype(int)\n",
    "            aws_data['Day'] = aws_data['Day'].astype(int)\n",
    "            \n",
    "            # Convert Day of Year to datetime\n",
    "            aws_data['date'] = pd.to_datetime(\n",
    "                aws_data['Year'].astype(str) + '-01-01'\n",
    "            ) + pd.to_timedelta(aws_data['Day'] - 1, unit='D')\n",
    "            \n",
    "            # Find albedo column\n",
    "            albedo_cols = [col for col in aws_data.columns if 'albedo' in col.lower()]\n",
    "            if albedo_cols:\n",
    "                albedo_col = albedo_cols[0]\n",
    "                aws_data['Albedo'] = pd.to_numeric(aws_data[albedo_col], errors='coerce')\n",
    "            else:\n",
    "                raise ValueError(f\"No albedo column found in Haig AWS data\")\n",
    "                \n",
    "        elif glacier_id == 'coropuna':\n",
    "            # Coropuna format: Timestamp, Albedo\n",
    "            aws_data = pd.read_csv(file_path)\n",
    "            aws_data['date'] = pd.to_datetime(aws_data['Timestamp'])\n",
    "            \n",
    "        elif glacier_id == 'athabasca':\n",
    "            # Athabasca format: Time, Albedo\n",
    "            aws_data = pd.read_csv(file_path)\n",
    "            aws_data['date'] = pd.to_datetime(aws_data['Time'])\n",
    "        \n",
    "        # Clean and validate data\n",
    "        aws_data = aws_data[['date', 'Albedo']].copy()\n",
    "        aws_data = aws_data.dropna(subset=['Albedo'])\n",
    "        aws_data = aws_data[aws_data['Albedo'] > 0]  # Remove invalid albedo values\n",
    "        aws_data = aws_data.drop_duplicates().sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        return aws_data\n",
    "\n",
    "print(\"✓ DataLoader class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pixel Selection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelSelector:\n",
    "    \"\"\"Implements intelligent pixel selection based on distance to AWS stations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        \n",
    "    def select_best_pixels(self, modis_data: pd.DataFrame, glacier_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Select best pixels for analysis based on AWS distance and glacier fraction.\"\"\"\n",
    "        logger.info(f\"Applying pixel selection for {glacier_id}...\")\n",
    "        \n",
    "        # Get AWS station coordinates\n",
    "        aws_station = self.config['aws_stations'][glacier_id]\n",
    "        aws_lat, aws_lon = aws_station['lat'], aws_station['lon']\n",
    "        \n",
    "        # Get available pixels with their quality metrics\n",
    "        pixel_summary = modis_data.groupby('pixel_id').agg({\n",
    "            'glacier_fraction': 'mean',\n",
    "            'albedo': 'count',\n",
    "            'latitude': 'first',\n",
    "            'longitude': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        pixel_summary.columns = ['pixel_id', 'avg_glacier_fraction', 'n_observations', 'latitude', 'longitude']\n",
    "        \n",
    "        # Apply quality filters\n",
    "        quality_filters = self.config['quality_filters']\n",
    "        quality_pixels = pixel_summary[\n",
    "            (pixel_summary['avg_glacier_fraction'] > quality_filters['min_glacier_fraction']) & \n",
    "            (pixel_summary['n_observations'] > quality_filters['min_observations'])\n",
    "        ].copy()\n",
    "        \n",
    "        if len(quality_pixels) == 0:\n",
    "            logger.warning(f\"No quality pixels found for {glacier_id}, using all data\")\n",
    "            return modis_data\n",
    "        \n",
    "        # Calculate distance to AWS station using Haversine formula\n",
    "        quality_pixels['distance_to_aws'] = self._haversine_distance(\n",
    "            quality_pixels['latitude'], quality_pixels['longitude'], aws_lat, aws_lon\n",
    "        )\n",
    "        \n",
    "        # For Athabasca (small dataset), use all pixels\n",
    "        if glacier_id == 'athabasca':\n",
    "            selected_pixel_ids = quality_pixels['pixel_id'].tolist()\n",
    "            logger.info(f\"Using all {len(selected_pixel_ids)} pixels for {glacier_id} (small dataset)\")\n",
    "        else:\n",
    "            # Sort by glacier fraction (descending) then distance (ascending)\n",
    "            quality_pixels = quality_pixels.sort_values([\n",
    "                'avg_glacier_fraction', 'distance_to_aws'\n",
    "            ], ascending=[False, True])\n",
    "            \n",
    "            # Select the best performing pixel\n",
    "            selected_pixels = quality_pixels.head(1)\n",
    "            selected_pixel_ids = selected_pixels['pixel_id'].tolist()\n",
    "            \n",
    "            logger.info(f\"Selected {len(selected_pixel_ids)} best pixel(s) for {glacier_id}\")\n",
    "            for _, pixel in selected_pixels.iterrows():\n",
    "                logger.info(f\"  Pixel {pixel['pixel_id']}: \"\n",
    "                           f\"glacier_fraction={pixel['avg_glacier_fraction']:.3f}, \"\n",
    "                           f\"distance={pixel['distance_to_aws']:.2f}km, \"\n",
    "                           f\"observations={pixel['n_observations']}\")\n",
    "        \n",
    "        # Filter MODIS data to selected pixels\n",
    "        filtered_data = modis_data[modis_data['pixel_id'].isin(selected_pixel_ids)].copy()\n",
    "        logger.info(f\"Filtered MODIS data from {len(modis_data)} to {len(filtered_data)} observations\")\n",
    "        \n",
    "        return filtered_data\n",
    "    \n",
    "    def _haversine_distance(self, lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate distance using Haversine formula.\"\"\"\n",
    "        R = 6371  # Earth's radius in km\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        return R * c\n",
    "\n",
    "print(\"✓ PixelSelector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Processing and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Handles AWS-MODIS data merging and statistical processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        \n",
    "    def merge_and_process(self, modis_data: pd.DataFrame, aws_data: pd.DataFrame, \n",
    "                         glacier_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Merge AWS and MODIS data and calculate statistics for each method.\"\"\"\n",
    "        logger.info(f\"Merging and processing data for {glacier_id}...\")\n",
    "        \n",
    "        results = []\n",
    "        methods = self.config['methods']\n",
    "        \n",
    "        for method in methods:\n",
    "            # Filter MODIS data for this method\n",
    "            method_data = modis_data[modis_data['method'] == method].copy()\n",
    "            \n",
    "            if len(method_data) == 0:\n",
    "                logger.warning(f\"No {method} data found for {glacier_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Merge with AWS data on date\n",
    "            merged = method_data.merge(aws_data, on='date', how='inner')\n",
    "            \n",
    "            if len(merged) < 3:  # Need minimum data points\n",
    "                logger.warning(f\"Insufficient {method} data for {glacier_id}: {len(merged)} points\")\n",
    "                continue\n",
    "            \n",
    "            # Apply outlier filtering\n",
    "            aws_clean, modis_clean = self._apply_outlier_filtering(\n",
    "                merged['Albedo'].values, merged['albedo'].values\n",
    "            )\n",
    "            \n",
    "            if len(aws_clean) < 3:\n",
    "                logger.warning(f\"Insufficient {method} data after outlier filtering for {glacier_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats = self._calculate_statistics(aws_clean, modis_clean)\n",
    "            \n",
    "            results.append({\n",
    "                'glacier_id': glacier_id,\n",
    "                'method': method,\n",
    "                'aws_values': aws_clean,\n",
    "                'modis_values': modis_clean,\n",
    "                **stats\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"Processed {method} for {glacier_id}: \"\n",
    "                       f\"{len(aws_clean)} samples, r={stats['correlation']:.3f}\")\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def _apply_outlier_filtering(self, aws_vals: np.ndarray, modis_vals: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Apply 2.5σ outlier filtering to AWS-MODIS pairs.\"\"\"\n",
    "        if len(aws_vals) < 3:\n",
    "            return aws_vals, modis_vals\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = modis_vals - aws_vals\n",
    "        mean_residual = np.mean(residuals)\n",
    "        std_residual = np.std(residuals)\n",
    "        \n",
    "        # Apply threshold\n",
    "        threshold = self.config['outlier_threshold'] * std_residual\n",
    "        mask = np.abs(residuals - mean_residual) <= threshold\n",
    "        \n",
    "        return aws_vals[mask], modis_vals[mask]\n",
    "    \n",
    "    def _calculate_statistics(self, aws_vals: np.ndarray, modis_vals: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Calculate comprehensive statistics between AWS and MODIS values.\"\"\"\n",
    "        if len(aws_vals) == 0:\n",
    "            return {\n",
    "                'correlation': np.nan, 'r_squared': np.nan, 'rmse': np.nan,\n",
    "                'mae': np.nan, 'bias': np.nan, 'n_samples': 0, 'p_value': 1.0\n",
    "            }\n",
    "        \n",
    "        # Basic statistics\n",
    "        correlation = np.corrcoef(aws_vals, modis_vals)[0, 1] if len(aws_vals) > 1 else np.nan\n",
    "        r_squared = correlation**2 if not np.isnan(correlation) else np.nan\n",
    "        \n",
    "        # Error metrics\n",
    "        residuals = modis_vals - aws_vals\n",
    "        rmse = np.sqrt(np.mean(residuals**2))\n",
    "        mae = np.mean(np.abs(residuals))\n",
    "        bias = np.mean(residuals)\n",
    "        \n",
    "        # Statistical significance\n",
    "        if len(aws_vals) > 2 and not np.isnan(correlation):\n",
    "            t_stat = correlation * np.sqrt((len(aws_vals) - 2) / (1 - correlation**2))\n",
    "            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), len(aws_vals) - 2))\n",
    "        else:\n",
    "            p_value = 1.0\n",
    "        \n",
    "        return {\n",
    "            'correlation': correlation if not np.isnan(correlation) else 0.0,\n",
    "            'r_squared': r_squared if not np.isnan(r_squared) else 0.0,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'bias': bias,\n",
    "            'n_samples': len(aws_vals),\n",
    "            'p_value': p_value\n",
    "        }\n",
    "\n",
    "print(\"✓ DataProcessor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScatterplotVisualizer:\n",
    "    \"\"\"Creates the 3×3 AWS vs MODIS scatterplot matrix.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        \n",
    "    def create_scatterplot_matrix(self, processed_data: List[pd.DataFrame], \n",
    "                                output_path: Optional[str] = None) -> plt.Figure:\n",
    "        \"\"\"Create the 3×3 scatterplot matrix.\"\"\"\n",
    "        logger.info(\"Creating AWS vs MODIS scatterplot matrix...\")\n",
    "        \n",
    "        # Set matplotlib style\n",
    "        try:\n",
    "            plt.style.use(self.config['visualization']['style'])\n",
    "        except:\n",
    "            logger.warning(\"Could not set plotting style, using default\")\n",
    "        \n",
    "        # Create figure and subplots\n",
    "        fig, axes = plt.subplots(3, 3, figsize=self.config['visualization']['figsize'])\n",
    "        \n",
    "        # Define layout: glaciers (rows) × methods (columns)\n",
    "        glaciers = ['athabasca', 'coropuna', 'haig']  # Match your image order\n",
    "        methods = self.config['methods']\n",
    "        \n",
    "        # Create title with pixel selection info\n",
    "        subtitle = \"Selected Best Pixels: 2/1/1 (Closest to AWS Stations)\"\n",
    "        fig.suptitle(f'AWS vs MODIS Albedo Correlations\\n{subtitle}', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Create scatterplots for each glacier-method combination\n",
    "        for i, glacier_id in enumerate(glaciers):\n",
    "            # Get data for this glacier\n",
    "            glacier_data = next((df for df in processed_data if df['glacier_id'].iloc[0] == glacier_id), pd.DataFrame())\n",
    "            \n",
    "            for j, method in enumerate(methods):\n",
    "                ax = axes[i, j]\n",
    "                \n",
    "                if not glacier_data.empty:\n",
    "                    # Get data for this method\n",
    "                    method_data = glacier_data[glacier_data['method'] == method]\n",
    "                    \n",
    "                    if not method_data.empty and len(method_data) > 0:\n",
    "                        row = method_data.iloc[0]\n",
    "                        aws_vals = row['aws_values']\n",
    "                        modis_vals = row['modis_values']\n",
    "                        \n",
    "                        # Create scatterplot\n",
    "                        self._create_single_scatterplot(ax, aws_vals, modis_vals, \n",
    "                                                      glacier_id, method, row)\n",
    "                    else:\n",
    "                        self._create_empty_plot(ax, glacier_id, method)\n",
    "                else:\n",
    "                    self._create_empty_plot(ax, glacier_id, method)\n",
    "                \n",
    "                # Set subplot title and labels\n",
    "                ax.set_title(f'{glacier_id.title()} - {method}', \n",
    "                           fontsize=12, fontweight='bold')\n",
    "                ax.set_xlabel('AWS Albedo', fontsize=10)\n",
    "                ax.set_ylabel('MODIS Albedo', fontsize=10)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.set_aspect('equal', adjustable='box')\n",
    "                ax.set_xlim(0, 1)\n",
    "                ax.set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(wspace=0.05)  # Make plots closer horizontally\n",
    "        \n",
    "        # Save figure if path provided\n",
    "        if output_path:\n",
    "            fig.savefig(output_path, dpi=self.config['visualization']['dpi'], \n",
    "                       bbox_inches='tight', facecolor='white')\n",
    "            logger.info(f\"Scatterplot matrix saved to: {output_path}\")\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _create_single_scatterplot(self, ax, aws_vals: np.ndarray, modis_vals: np.ndarray,\n",
    "                                 glacier_id: str, method: str, stats_row: pd.Series):\n",
    "        \"\"\"Create a single scatterplot with statistics.\"\"\"\n",
    "        # Get glacier color\n",
    "        color = self.config['colors'][glacier_id]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        ax.scatter(aws_vals, modis_vals, alpha=0.6, s=20, color=color)\n",
    "        \n",
    "        # Add 1:1 reference line\n",
    "        ax.plot([0, 1], [0, 1], 'k--', alpha=0.7, linewidth=1, label='1:1 line')\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(aws_vals) > 1:\n",
    "            z = np.polyfit(aws_vals, modis_vals, 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax.plot(aws_vals, p(aws_vals), 'r-', alpha=0.8, linewidth=1.5)\n",
    "        \n",
    "        # Add statistics text\n",
    "        stats_text = (f'R = {stats_row[\"correlation\"]:.3f}\\n'\n",
    "                     f'R² = {stats_row[\"r_squared\"]:.3f}\\n'\n",
    "                     f'RMSE = {stats_row[\"rmse\"]:.3f}\\n'\n",
    "                     f'MAE = {stats_row[\"mae\"]:.3f}\\n'\n",
    "                     f'Bias = {stats_row[\"bias\"]:.3f}\\n'\n",
    "                     f'n = {stats_row[\"n_samples\"]}')\n",
    "        \n",
    "        ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, \n",
    "               verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9),\n",
    "               fontsize=8)\n",
    "    \n",
    "    def _create_empty_plot(self, ax, glacier_id: str, method: str):\n",
    "        \"\"\"Create empty plot for missing data.\"\"\"\n",
    "        ax.text(0.5, 0.5, 'No data available', transform=ax.transAxes,\n",
    "               ha='center', va='center', fontsize=10, style='italic')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "print(\"✓ ScatterplotVisualizer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Processing Workflow\n",
    "\n",
    "Now let's run the complete analysis pipeline for all three glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all processing components\n",
    "data_loader = DataLoader(CONFIG)\n",
    "pixel_selector = PixelSelector(CONFIG)\n",
    "data_processor = DataProcessor(CONFIG)\n",
    "visualizer = ScatterplotVisualizer(CONFIG)\n",
    "\n",
    "print(\"✓ All processing components initialized\")\n",
    "print(\"Ready to process glacier data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each glacier\n",
    "all_processed_data = []\n",
    "processing_summary = []\n",
    "\n",
    "for glacier_id in ['athabasca', 'haig', 'coropuna']:\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing {glacier_id.upper()} Glacier\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Load data\n",
    "        modis_data, aws_data = data_loader.load_glacier_data(glacier_id)\n",
    "        \n",
    "        # Apply pixel selection\n",
    "        selected_modis = pixel_selector.select_best_pixels(modis_data, glacier_id)\n",
    "        \n",
    "        # Process and merge data\n",
    "        processed = data_processor.merge_and_process(selected_modis, aws_data, glacier_id)\n",
    "        \n",
    "        if not processed.empty:\n",
    "            all_processed_data.append(processed)\n",
    "            processing_summary.append({\n",
    "                'glacier': glacier_id,\n",
    "                'status': 'SUCCESS',\n",
    "                'methods_processed': len(processed),\n",
    "                'total_modis_obs': len(modis_data),\n",
    "                'selected_modis_obs': len(selected_modis),\n",
    "                'aws_obs': len(aws_data)\n",
    "            })\n",
    "            print(f\"✓ Successfully processed {glacier_id}: {len(processed)} methods\")\n",
    "        else:\n",
    "            processing_summary.append({\n",
    "                'glacier': glacier_id,\n",
    "                'status': 'FAILED',\n",
    "                'methods_processed': 0,\n",
    "                'error': 'No processed data'\n",
    "            })\n",
    "            print(f\"✗ No processed data for {glacier_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        processing_summary.append({\n",
    "            'glacier': glacier_id,\n",
    "            'status': 'ERROR',\n",
    "            'methods_processed': 0,\n",
    "            'error': str(e)\n",
    "        })\n",
    "        print(f\"✗ Error processing {glacier_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Display processing summary\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"PROCESSING SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "summary_df = pd.DataFrame(processing_summary)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "successful_glaciers = len([s for s in processing_summary if s['status'] == 'SUCCESS'])\n",
    "print(f\"\\n✓ Successfully processed {successful_glaciers} out of 3 glaciers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate the Scatterplot Matrix\n",
    "\n",
    "Now let's create the publication-ready 3×3 scatterplot matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatterplot matrix\n",
    "if all_processed_data:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Creating Scatterplot Matrix\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Generate output filename\n",
    "    output_path = \"aws_vs_modis_scatterplot_matrix.png\"\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = visualizer.create_scatterplot_matrix(all_processed_data, output_path)\n",
    "    \n",
    "    # Display the plot in notebook\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ SUCCESS: Scatterplot matrix generated and saved to {output_path}\")\n",
    "    print(f\"✓ Total glaciers in visualization: {len(all_processed_data)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✗ ERROR: No data could be processed for any glacier\")\n",
    "    print(\"Please check the file paths in the configuration section above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Results Analysis\n",
    "\n",
    "Let's examine the detailed statistics for each glacier-method combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed results table\n",
    "if all_processed_data:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED STATISTICAL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Combine all processed data\n",
    "    all_stats = []\n",
    "    for df in all_processed_data:\n",
    "        for _, row in df.iterrows():\n",
    "            all_stats.append({\n",
    "                'Glacier': row['glacier_id'].title(),\n",
    "                'Method': row['method'],\n",
    "                'n': int(row['n_samples']),\n",
    "                'R': f\"{row['correlation']:.3f}\",\n",
    "                'R²': f\"{row['r_squared']:.3f}\",\n",
    "                'RMSE': f\"{row['rmse']:.3f}\",\n",
    "                'MAE': f\"{row['mae']:.3f}\",\n",
    "                'Bias': f\"{row['bias']:.3f}\",\n",
    "                'p-value': f\"{row['p_value']:.4f}\"\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(all_stats)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Find best performing combinations\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BEST PERFORMING COMBINATIONS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Convert back to numeric for analysis\n",
    "    numeric_df = pd.DataFrame(all_stats)\n",
    "    numeric_df['R_num'] = numeric_df['R'].astype(float)\n",
    "    numeric_df['RMSE_num'] = numeric_df['RMSE'].astype(float)\n",
    "    \n",
    "    # Best correlation\n",
    "    best_r = numeric_df.loc[numeric_df['R_num'].idxmax()]\n",
    "    print(f\"Highest Correlation: {best_r['Glacier']} - {best_r['Method']} (R = {best_r['R']})\")\n",
    "    \n",
    "    # Best RMSE (lowest)\n",
    "    best_rmse = numeric_df.loc[numeric_df['RMSE_num'].idxmin()]\n",
    "    print(f\"Lowest RMSE: {best_rmse['Glacier']} - {best_rmse['Method']} (RMSE = {best_rmse['RMSE']})\")\n",
    "    \n",
    "    # Summary by glacier\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY BY GLACIER\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for glacier in ['Athabasca', 'Haig', 'Coropuna']:\n",
    "        glacier_data = numeric_df[numeric_df['Glacier'] == glacier]\n",
    "        if not glacier_data.empty:\n",
    "            avg_r = glacier_data['R_num'].mean()\n",
    "            avg_rmse = glacier_data['RMSE_num'].mean()\n",
    "            total_n = glacier_data['n'].sum()\n",
    "            print(f\"{glacier}: Avg R = {avg_r:.3f}, Avg RMSE = {avg_rmse:.3f}, Total n = {total_n}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configuration Tips\n",
    "\n",
    "### Modifying File Paths\n",
    "If your data files are in different locations, update the `CONFIG['data_paths']` section at the top of this notebook.\n",
    "\n",
    "### Adjusting Parameters\n",
    "- **Outlier threshold**: Modify `CONFIG['outlier_threshold']` (default: 2.5σ)\n",
    "- **Quality filters**: Adjust `CONFIG['quality_filters']['min_glacier_fraction']` and `min_observations`\n",
    "- **Colors**: Change glacier colors in `CONFIG['colors']`\n",
    "- **Figure size**: Modify `CONFIG['visualization']['figsize']`\n",
    "\n",
    "### Adding New Glaciers\n",
    "To add a new glacier:\n",
    "1. Add data paths to `CONFIG['data_paths']`\n",
    "2. Add AWS station coordinates to `CONFIG['aws_stations']`\n",
    "3. Add color to `CONFIG['colors']`\n",
    "4. Update the glacier list in the processing loops\n",
    "\n",
    "### Troubleshooting\n",
    "- **File not found errors**: Check that all file paths in CONFIG are correct\n",
    "- **No data processed**: Verify data file formats match expected structure\n",
    "- **Missing methods**: Check that your MODIS data contains the expected method columns\n",
    "- **Empty plots**: Ensure adequate data overlap between AWS and MODIS observations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}