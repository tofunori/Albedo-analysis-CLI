#!/usr/bin/env python3
"""
Unified MODIS Albedo Analysis Engine

This engine combines the functionality from both main.py (basic analysis) and 
pivot_based_main.py (enhanced analysis with comprehensive visualization suite) 
into a single, flexible analysis system.

Key Features:
- Multiple analysis modes: auto, basic, enhanced, comprehensive
- Pixel selection algorithms (distance + glacier fraction weighting)
- 7-plot visualization suite for enhanced modes
- Outlier detection with residual-based methods
- Comprehensive statistical analysis
- Spatial mapping with pixel highlighting
- Publication-ready outputs

Usage:
    python albedo_analysis_engine.py --glacier haig --analysis-mode enhanced --selected-pixels
    python albedo_analysis_engine.py --all-glaciers --analysis-mode auto
"""

import argparse
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from scipy import stats as scipy_stats
from typing import Dict, Any, List, Tuple, Optional
import sys
import os

# Add root to path to access new module structure
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from utils.config.helpers import load_config, setup_logging, ensure_directory_exists, get_timestamp
from data_processing.loaders.pivot_loaders import (
    create_pivot_based_loader, 
    AthabascaMultiProductLoader,
    AthabascaAWSLoader
)
from data_processing.processors.pivot_processor import PivotBasedProcessor
from data_processing.processors.data_processor import DataProcessor
from analysis.core.albedo_calculator import AlbedoCalculator
from spatial_analysis.coordinates.spatial_utils import SpatialProcessor
from spatial_analysis.masks.glacier_masks import GlacierMaskProcessor
from visualization.plots.statistical_plots import PlotGenerator


class AlbedoAnalysisEngine:
    """
    Unified analysis engine for MODIS albedo analysis.
    
    Supports multiple analysis modes:
    - 'auto': Automatically determines mode based on glacier configuration
    - 'basic': Basic analysis with standard outputs (original main.py functionality)
    - 'enhanced': Advanced analysis with pivot-based methodology (pivot_based_main.py functionality)
    - 'comprehensive': Enhanced analysis + full 7-plot visualization suite
    """
    
    def __init__(self, config_path: str):
        """Initialize the unified analysis engine."""
        self.config = load_config(config_path)
        setup_logging(self.config)
        self.logger = logging.getLogger(__name__)
        
        # Initialize all processors (both basic and advanced)
        self.data_processor = DataProcessor(self.config)
        self.pivot_processor = PivotBasedProcessor(self.config)
        self.albedo_calculator = AlbedoCalculator(self.config)
        self.spatial_processor = SpatialProcessor(self.config)
        self.mask_processor = GlacierMaskProcessor(self.config)
        
        # Initialize visualization components
        try:
            self.plot_generator = PlotGenerator(self.config)
        except ImportError as e:
            self.logger.warning(f"PlotGenerator failed to initialize: {e}")
            self.plot_generator = None
        
        # Set up output directories
        self._setup_output_directories()
        
    def _setup_output_directories(self):
        """Create necessary output directories."""
        output_paths = [
            self.config['output']['results_path'],
            self.config['output']['plots_path'],
            self.config['output']['maps_path']
        ]
        
        for path in output_paths:
            ensure_directory_exists(path)
    
    def process_glacier(self, glacier_id: str, 
                       analysis_mode: str = 'auto',
                       use_selected_pixels: bool = False) -> Dict[str, Any]:
        """
        Process a single glacier using the specified analysis mode.
        
        Args:
            glacier_id: Identifier for the glacier to process
            analysis_mode: 'auto', 'basic', 'enhanced', or 'comprehensive'
            use_selected_pixels: Whether to use pixel selection algorithm
            
        Returns:
            Dictionary containing analysis results and metadata
        """
        self.logger.info(f"Starting analysis for glacier: {glacier_id}")
        self.logger.info(f"Analysis mode: {analysis_mode}, Selected pixels: {use_selected_pixels}")
        
        try:
            # Load glacier configuration
            glacier_config = self._load_glacier_config(glacier_id)
            
            # Determine actual analysis mode
            actual_mode = self._determine_analysis_mode(analysis_mode, glacier_config)
            self.logger.info(f"Using analysis mode: {actual_mode}")
            
            # Create glacier-specific output directories
            glacier_output_dir = self._create_glacier_output_dir(glacier_id, actual_mode)
            
            # Route to appropriate analysis method
            if actual_mode == 'basic':
                return self._process_basic_analysis(glacier_id, glacier_config, glacier_output_dir)
            else:
                # enhanced or comprehensive modes use pivot-based approach
                return self._process_enhanced_analysis(
                    glacier_id, glacier_config, glacier_output_dir, 
                    actual_mode, use_selected_pixels
                )
                
        except Exception as e:
            self.logger.error(f"Error processing glacier {glacier_id}: {e}")
            raise
    
    def _determine_analysis_mode(self, requested_mode: str, glacier_config: Dict[str, Any]) -> str:
        """Determine the actual analysis mode to use."""
        if requested_mode == 'auto':
            # Auto mode: use enhanced for athabasca_multiproduct types, basic otherwise
            if glacier_config.get('data_type') == 'athabasca_multiproduct':
                return 'comprehensive'  # Use full visualization suite for enhanced data types
            else:
                return 'basic'
        else:
            return requested_mode
    
    def _load_glacier_config(self, glacier_id: str) -> Dict[str, Any]:
        """Load configuration for specific glacier."""
        glacier_sites_config = load_config('config/glacier_sites.yaml')
        
        if glacier_id not in glacier_sites_config['glaciers']:
            raise ValueError(f"Glacier {glacier_id} not found in configuration")
        
        return glacier_sites_config['glaciers'][glacier_id]
    
    def _create_glacier_output_dir(self, glacier_id: str, analysis_mode: str) -> str:
        """Create output directory for specific glacier."""
        timestamp = get_timestamp()
        
        # Include analysis mode in directory name for clarity
        if analysis_mode == 'basic':
            glacier_dir = os.path.join(self.config['output']['base_path'], f"{glacier_id}_basic_{timestamp}")
        else:
            glacier_dir = os.path.join(self.config['output']['base_path'], f"{glacier_id}_{analysis_mode}_{timestamp}")
        
        subdirs = ['results', 'plots', 'maps']
        for subdir in subdirs:
            ensure_directory_exists(os.path.join(glacier_dir, subdir))
        
        return glacier_dir
    
    def _process_basic_analysis(self, glacier_id: str, glacier_config: Dict[str, Any], 
                               output_dir: str) -> Dict[str, Any]:
        """Process glacier using basic analysis pipeline (original main.py functionality)."""
        self.logger.info("Using basic analysis pipeline...")
        
        # Step 1: Load and validate data
        self.logger.info("Loading data...")
        data_dict = self._load_basic_data(glacier_config)
        
        # Step 2: Spatial processing (if data available)
        self.logger.info("Processing spatial data...")
        spatial_results = self._process_spatial_data(data_dict, glacier_config)
        
        # Step 3: Temporal alignment
        self.logger.info("Aligning temporal data...")
        aligned_data = self._align_temporal_data(data_dict)
        
        # Step 4: Statistical analysis
        self.logger.info("Performing statistical analysis...")
        statistics = self._perform_basic_statistical_analysis(aligned_data)
        
        # Step 5: Generate basic visualizations
        self.logger.info("Generating basic plots...")
        self._generate_basic_plots(aligned_data, statistics, glacier_id, output_dir)
        
        # Step 6: Generate basic maps
        self.logger.info("Generating basic maps...")
        self._generate_basic_maps(spatial_results, glacier_config, glacier_id, output_dir)
        
        # Step 7: Export results
        self.logger.info("Exporting results...")
        results = self._export_basic_results(statistics, glacier_id, output_dir)
        
        self.logger.info(f"Basic analysis completed for glacier: {glacier_id}")
        return results
    
    def _process_enhanced_analysis(self, glacier_id: str, glacier_config: Dict[str, Any], 
                                 output_dir: str, analysis_mode: str, 
                                 use_selected_pixels: bool) -> Dict[str, Any]:
        """Process glacier using enhanced analysis pipeline (pivot_based_main.py functionality)."""
        self.logger.info(f"Using enhanced analysis pipeline (mode: {analysis_mode})...")
        
        # Step 1: Load data using pivot-based loaders
        self.logger.info("Loading data using pivot-based approach...")
        modis_data, aws_data = self._load_pivot_based_data(glacier_config, glacier_id, use_selected_pixels)
        
        # Step 2: Apply Terra/Aqua merging
        self.logger.info("Applying Terra/Aqua merge...")
        modis_merged = self.pivot_processor.apply_terra_aqua_merge(modis_data)
        
        # Step 3: Create pivot table and merge with AWS
        self.logger.info("Creating pivot table and merging with AWS...")
        merged_data = self.pivot_processor.create_pivot_and_merge(modis_merged, aws_data)
        
        # Step 4: Statistical analysis
        self.logger.info("Performing enhanced statistical analysis...")
        statistics = self._perform_enhanced_statistical_analysis(merged_data)
        
        # Step 5: Outlier analysis
        self.logger.info("Performing outlier analysis...")
        outlier_stats = self._perform_outlier_analysis(merged_data)
        statistics['outlier_analysis'] = outlier_stats
        
        # Step 6: Generate visualizations (always generate ALL plots like before)
        plot_config = self.config.get('visualization', {}).get('plot_output', {})
        plot_mode = plot_config.get('plot_mode', 'both')
        
        # Use the ORIGINAL enhanced plotting system that worked before
        self.logger.info("Generating enhanced plots...")
        try:
            self._generate_enhanced_plots(merged_data, statistics, glacier_id, output_dir)
        except Exception as e:
            self.logger.error(f"Error in enhanced plots: {e}")
            # Fallback to user style
            try:
                self._generate_user_style_plots(merged_data, statistics, glacier_id, output_dir)
            except Exception as fallback_error:
                self.logger.error(f"Fallback plotting also failed: {fallback_error}")
        
        # Step 7: Export results
        self.logger.info("Exporting enhanced results...")
        results = self._export_enhanced_results(statistics, merged_data, glacier_id, output_dir, analysis_mode)
        
        self.logger.info(f"Enhanced analysis completed for glacier: {glacier_id}")
        self.logger.info(f"Successfully processed {len(merged_data)} merged observations")
        
        return results
    
    def _generate_all_original_plots(self, merged_data: pd.DataFrame, statistics: Dict[str, Any], 
                                   glacier_id: str, output_dir: str):
        """Generate all 7 original plots exactly like your working examples."""
        plots_dir = os.path.join(output_dir, 'plots')
        
        # Initialize plot generator
        from visualization.plots.statistical_plots import PlotGenerator
        import matplotlib.pyplot as plt
        plot_generator = PlotGenerator(self.config)
        
        modis_methods = [col for col in merged_data.columns if col in ['MCD43A3', 'MOD09GA', 'MOD10A1']]
        
        if not modis_methods or 'AWS' not in merged_data.columns:
            self.logger.warning("Insufficient data for plotting")
            return
        
        glacier_name = glacier_id.title()
        
        # Extract AWS and MODIS data
        aws_data = merged_data['AWS'].dropna()
        modis_data = {method: merged_data[method].dropna() for method in modis_methods}
        
        # Prepare melted data for time series and seasonal analysis
        melted_data = None
        self.logger.info(f"Available columns in merged_data: {list(merged_data.columns)}")
        if 'date' in merged_data.columns:
            value_cols = ['AWS'] + modis_methods
            melted_data = merged_data[['date'] + value_cols].melt(
                id_vars=['date'], value_vars=value_cols, 
                var_name='method', value_name='albedo'
            ).dropna()
            self.logger.info(f"Created melted_data with {len(melted_data)} rows")
        else:
            self.logger.warning(f"No 'date' column found in merged_data. Available columns: {list(merged_data.columns)}")
        
        # 1. Original Outlier Analysis (6-panel before/after outlier removal)
        outlier_path = os.path.join(plots_dir, f"{glacier_id}_01_outlier_analysis.png")
        try:
            self.logger.info("Creating original outlier analysis (6-panel)")
            fig = plot_generator.create_original_outlier_analysis(
                aws_data, modis_data, glacier_name, outlier_threshold=2.5, output_path=outlier_path
            )
            if fig:
                plt.close(fig)
                self.logger.info(f"Generated original outlier analysis: {outlier_path}")
        except Exception as e:
            self.logger.error(f"Error creating original outlier analysis: {e}")
        
        # 2. Four Metrics Boxplot Summary (2x2 layout: correlation, bias, MAE, RMSE)
        summary_path = os.path.join(plots_dir, f"{glacier_id}_02_comprehensive_summary_figure.png")
        try:
            self.logger.info("Creating 2x2 metrics boxplot summary")
            fig = plot_generator.create_four_metrics_boxplot_summary(
                aws_data, modis_data, glacier_name, summary_path
            )
            if fig:
                plt.close(fig)
                self.logger.info(f"Generated 2x2 metrics boxplot summary: {summary_path}")
        except Exception as e:
            self.logger.error(f"Error creating 2x2 metrics boxplot summary: {e}")
        
        # 3. Original Seasonal Analysis (4-panel monthly boxplots)
        seasonal_path = os.path.join(plots_dir, f"{glacier_id}_03_seasonal_comparison.png")
        try:
            if melted_data is not None and len(melted_data) > 0:
                self.logger.info("Creating original seasonal analysis (4-panel)")
                fig = plot_generator.create_original_seasonal_analysis(
                    melted_data, 'date', 'albedo', 'method', glacier_name, seasonal_path
                )
                if fig:
                    plt.close(fig)
                    self.logger.info(f"Generated original seasonal analysis: {seasonal_path}")
            else:
                self.logger.warning("No melted data available for seasonal analysis")
        except Exception as e:
            self.logger.error(f"Error creating original seasonal analysis: {e}")
        
        # 4. Time Series Analysis
        timeseries_path = os.path.join(plots_dir, f"{glacier_id}_04_time_series_analysis.png")
        try:
            if melted_data is not None and len(melted_data) > 0:
                self.logger.info("Creating time series analysis")
                fig = plot_generator.create_time_series_plot(
                    melted_data, 'date', 'albedo', 'method', 
                    f"{glacier_name} - Time Series Analysis", timeseries_path
                )
                if fig:
                    plt.close(fig)
                    self.logger.info(f"Generated time series analysis: {timeseries_path}")
            else:
                self.logger.warning("No melted data available for time series analysis")
        except Exception as e:
            self.logger.error(f"Error creating time series analysis: {e}")
        
        # 5. Original Correlation & Bias Analysis (4-panel comprehensive)
        bias_analysis_path = os.path.join(plots_dir, f"{glacier_id}_05_correlation_bias_analysis.png")
        try:
            self.logger.info("Creating original correlation and bias analysis (4-panel)")
            fig = plot_generator.create_original_correlation_bias_analysis(
                aws_data, modis_data, glacier_name, bias_analysis_path
            )
            if fig:
                plt.close(fig)
                self.logger.info(f"Generated original correlation & bias analysis: {bias_analysis_path}")
        except Exception as e:
            self.logger.error(f"Error creating original correlation & bias analysis: {e}")
        
        self.logger.info(f"Completed generating 5 plots for {glacier_id} (replaced 2 plots with single 2x2 metrics boxplot)")
    
    # ===== BASIC ANALYSIS METHODS =====
    
    def _load_basic_data(self, glacier_config: Dict[str, Any]) -> Dict[str, Any]:
        """Load data for basic analysis."""
        data_dict = {}
        
        # Load MODIS data
        modis_products = self.config['analysis']['albedo']['modis_products']
        data_dict['modis'] = {}
        
        for product in modis_products:
            if product in glacier_config['data_files']['modis']:
                file_path = os.path.join(
                    self.config['data']['modis_path'],
                    glacier_config['data_files']['modis'][product]
                )
                
                if os.path.exists(file_path):
                    try:
                        loader = create_pivot_based_loader(product, self.config, glacier_config)
                        data_dict['modis'][product] = loader.load_data(file_path)
                        self.logger.info(f"Loaded {product}: {len(data_dict['modis'][product])} records")
                    except ValueError:
                        # Fallback to enhanced loader for non-pivot methods
                        from data_processing.loaders.enhanced_loaders import create_enhanced_loader
                        loader = create_enhanced_loader(product, self.config, glacier_config)
                        data_dict['modis'][product] = loader.load_data(file_path)
                        self.logger.info(f"Loaded {product}: {len(data_dict['modis'][product])} records")
                else:
                    self.logger.warning(f"MODIS file not found: {file_path}")
        
        # Load AWS data
        aws_file_path = os.path.join(
            self.config['data']['aws_path'],
            glacier_config['data_files']['aws']
        )
        
        if os.path.exists(aws_file_path):
            try:
                aws_loader = create_pivot_based_loader('AWS', self.config, None)
                data_dict['aws'] = aws_loader.load_data(aws_file_path)
            except ValueError:
                # Fallback to enhanced loader
                from data_processing.loaders.enhanced_loaders import create_enhanced_loader
                aws_loader = create_enhanced_loader('AWS', self.config, glacier_config)
                data_dict['aws'] = aws_loader.load_data(aws_file_path)
            
            self.logger.info(f"Loaded AWS data: {len(data_dict['aws'])} records")
        else:
            self.logger.warning(f"AWS file not found: {aws_file_path}")
            data_dict['aws'] = pd.DataFrame()
        
        return data_dict
    
    def _process_spatial_data(self, data_dict: Dict[str, Any], glacier_config: Dict[str, Any]) -> Dict[str, Any]:
        """Process spatial data for basic analysis."""
        spatial_results = {}
        
        try:
            # Load glacier mask if available
            if 'mask' in glacier_config.get('data_files', {}):
                mask_file = glacier_config['data_files']['mask']
                spatial_results['glacier_mask'] = self.mask_processor.load_mask(mask_file)
            
            # Process AWS station locations
            if 'aws_stations' in glacier_config:
                spatial_results['aws_stations'] = glacier_config['aws_stations']
            
        except Exception as e:
            self.logger.warning(f"Error processing spatial data: {e}")
            spatial_results = {}
        
        return spatial_results
    
    def _align_temporal_data(self, data_dict: Dict[str, Any]) -> pd.DataFrame:
        """Align temporal data for basic analysis."""
        try:
            return self.data_processor.align_temporal_data(data_dict)
        except Exception as e:
            self.logger.error(f"Error aligning temporal data: {e}")
            return pd.DataFrame()
    
    def _perform_basic_statistical_analysis(self, aligned_data: pd.DataFrame) -> Dict[str, Any]:
        """Perform basic statistical analysis."""
        statistics = {}
        
        if aligned_data.empty:
            return statistics
        
        try:
            # Calculate basic metrics
            statistics['method_comparison'] = {}
            modis_products = self.config['analysis']['albedo']['modis_products']
            
            for product in modis_products:
                if product in aligned_data.columns and 'AWS' in aligned_data.columns:
                    valid_data = aligned_data[[product, 'AWS']].dropna()
                    
                    if len(valid_data) > 0:
                        aws_vals = valid_data['AWS']
                        modis_vals = valid_data[product]
                        
                        # Basic metrics
                        r, p = scipy_stats.pearsonr(modis_vals, aws_vals)
                        rmse = np.sqrt(np.mean((modis_vals - aws_vals)**2))
                        mae = np.mean(np.abs(modis_vals - aws_vals))
                        bias = np.mean(modis_vals - aws_vals)
                        
                        statistics['method_comparison'][product] = {
                            'n_samples': len(valid_data),
                            'correlation': r,
                            'p_value': p,
                            'rmse': rmse,
                            'mae': mae,
                            'bias': bias
                        }
                        
                        self.logger.info(f"{product}: n={len(valid_data)}, r={r:.3f}, RMSE={rmse:.3f}")
        
        except Exception as e:
            self.logger.error(f"Error in basic statistical analysis: {e}")
        
        return statistics
    
    def _generate_basic_plots(self, aligned_data: pd.DataFrame, statistics: Dict[str, Any], 
                            glacier_id: str, output_dir: str):
        """Generate basic plots for basic analysis mode."""
        if self.plot_generator:
            try:
                plots_dir = os.path.join(output_dir, 'plots')
                self.plot_generator.generate_comparison_plots(
                    aligned_data, statistics, glacier_id, plots_dir
                )
            except Exception as e:
                self.logger.error(f"Error generating basic plots: {e}")
    
    def _generate_basic_maps(self, spatial_results: Dict[str, Any], glacier_config: Dict[str, Any],
                           glacier_id: str, output_dir: str):
        """Generate basic maps for basic analysis mode."""
        try:
            maps_dir = os.path.join(output_dir, 'maps')
            # Basic map generation would go here
            self.logger.info(f"Basic maps would be generated in: {maps_dir}")
        except Exception as e:
            self.logger.error(f"Error generating basic maps: {e}")
    
    def _export_basic_results(self, statistics: Dict[str, Any], glacier_id: str, 
                            output_dir: str) -> Dict[str, Any]:
        """Export results for basic analysis."""
        results_dir = os.path.join(output_dir, 'results')
        
        # Export basic statistics
        if 'method_comparison' in statistics:
            stats_file = os.path.join(results_dir, f"{glacier_id}_basic_method_comparison.csv")
            stats_df = pd.DataFrame(statistics['method_comparison']).T
            stats_df.to_csv(stats_file)
            self.logger.info(f"Exported basic results: {stats_file}")
        
        return {
            'glacier_id': glacier_id,
            'analysis_type': 'basic',
            'analysis_timestamp': get_timestamp(),
            'output_directory': output_dir,
            'statistics': statistics
        }
    
    # ===== ENHANCED ANALYSIS METHODS (from pivot_based_main.py) =====
    
    def _load_pivot_based_data(self, glacier_config: Dict[str, Any], glacier_id: str, 
                              use_selected_pixels: bool) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load data using pivot-based loaders (from pivot_based_main.py)."""
        
        # Determine search paths based on glacier
        glacier_name = glacier_config['name'].lower().split()[0].lower()
        search_paths = [self.config['data']['modis_path']]
        
        # Add glacier-specific paths
        if glacier_name == 'athabasca' and 'athabasca_modis_path' in self.config['data']:
            search_paths.insert(0, self.config['data']['athabasca_modis_path'])
        elif glacier_name == 'haig' and 'haig_modis_path' in self.config['data']:
            search_paths.insert(0, self.config['data']['haig_modis_path'])
        elif glacier_name == 'coropuna' and 'coropuna_modis_path' in self.config['data']:
            search_paths.insert(0, self.config['data']['coropuna_modis_path'])
        
        multiproduct_file = None
        
        # Search for MultiProduct file in all paths
        for search_path in search_paths:
            if not os.path.exists(search_path):
                continue
                
            for filename in os.listdir(search_path):
                if (('MultiProduct' in filename and 
                     (glacier_name in filename.lower() or 
                      any(keyword in filename.lower() for keyword in ['multiproduct', 'multi_product']))) or
                    (glacier_name == 'coropuna' and 'coropuna_glacier' in filename.lower() and filename.endswith('.csv'))):
                    multiproduct_file = os.path.join(search_path, filename)
                    self.logger.info(f"Found MultiProduct file: {multiproduct_file}")
                    break
            
            if multiproduct_file:
                break
        
        if not multiproduct_file:
            raise FileNotFoundError(f"MultiProduct file not found for {glacier_config['name']} in paths: {search_paths}")
        
        # Load MODIS data
        modis_loader = create_pivot_based_loader("athabasca_multiproduct", self.config, glacier_config)
        modis_data = modis_loader.load_data(multiproduct_file)
        
        # Store original MODIS data for spatial mapping
        self._last_modis_data_original = modis_data.copy()
        
        # Apply pixel selection if requested
        if use_selected_pixels:
            self.logger.info(f"Applying pixel selection for {glacier_name}...")
            modis_data = self._apply_pixel_selection(modis_data, glacier_id, glacier_config)
            self._last_modis_data_selected = modis_data.copy()
            self._use_selected_pixels = True
        else:
            self._last_modis_data_selected = modis_data.copy()
            self._use_selected_pixels = False
        
        # Load AWS data
        aws_loader = create_pivot_based_loader("AWS", self.config, None)
        
        if 'aws' in glacier_config.get('data_files', {}):
            # Determine AWS search paths
            aws_search_paths = [self.config['data']['aws_path']]
            if glacier_name == 'athabasca' and 'athabasca_aws_path' in self.config['data']:
                aws_search_paths.insert(0, self.config['data']['athabasca_aws_path'])
            elif glacier_name == 'haig' and 'haig_aws_path' in self.config['data']:
                aws_search_paths.insert(0, self.config['data']['haig_aws_path'])
            elif glacier_name == 'coropuna' and 'coropuna_aws_path' in self.config['data']:
                aws_search_paths.insert(0, self.config['data']['coropuna_aws_path'])
            
            aws_file_path = None
            aws_filename = glacier_config['data_files']['aws']
            
            for aws_path in aws_search_paths:
                potential_path = os.path.join(aws_path, aws_filename)
                if os.path.exists(potential_path):
                    aws_file_path = potential_path
                    self.logger.info(f"Found AWS file: {aws_file_path}")
                    break
            
            if aws_file_path:
                aws_data = aws_loader.load_data(aws_file_path)
            else:
                raise FileNotFoundError(f"AWS file '{aws_filename}' not found in paths: {aws_search_paths}")
        else:
            self.logger.warning("No separate AWS file specified, trying integrated AWS")
            aws_data = aws_loader.load_data(multiproduct_file)
        
        self.logger.info(f"Loaded MODIS data: {len(modis_data)} records")
        self.logger.info(f"Loaded AWS data: {len(aws_data)} records")
        
        return modis_data, aws_data
    
    def _apply_pixel_selection(self, modis_data: pd.DataFrame, glacier_id: str, 
                              glacier_config: Dict[str, Any]) -> pd.DataFrame:
        """Apply pixel selection algorithm (from pivot_based_main.py)."""
        try:
            # Get AWS station coordinates
            aws_stations = glacier_config.get('aws_stations', {})
            if not aws_stations:
                self.logger.warning(f"No AWS stations found for {glacier_id}, using standard selection")
                return self._apply_standard_pixel_selection(modis_data, glacier_id)
            
            # Use first AWS station coordinates
            aws_station = list(aws_stations.values())[0]
            aws_lat = aws_station['lat']
            aws_lon = aws_station['lon']
            
            # Get available pixels with their quality metrics and coordinates
            pixel_summary = modis_data.groupby('pixel_id').agg({
                'glacier_fraction': 'mean',
                'albedo': 'count',
                'ndsi': 'mean',
                'latitude': 'first',
                'longitude': 'first'
            }).reset_index()
            
            pixel_summary.columns = ['pixel_id', 'avg_glacier_fraction', 'n_observations', 'avg_ndsi', 'latitude', 'longitude']
            
            # Filter pixels based on quality criteria
            quality_pixels = pixel_summary[
                (pixel_summary['avg_glacier_fraction'] > 0.1) & 
                (pixel_summary['n_observations'] > 10)
            ].copy()
            
            if len(quality_pixels) == 0:
                self.logger.warning(f"No quality pixels found for {glacier_id}, using all data")
                return modis_data
            
            # Calculate distance using Haversine formula
            def haversine_distance(lat1, lon1, lat2, lon2):
                R = 6371  # Earth's radius in km
                lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
                dlat = lat2 - lat1
                dlon = lon2 - lon1
                a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
                c = 2 * np.arcsin(np.sqrt(a))
                return R * c
            
            quality_pixels['distance_to_aws'] = haversine_distance(
                quality_pixels['latitude'], quality_pixels['longitude'], aws_lat, aws_lon
            )
            
            # Sort by performance first, then distance
            quality_pixels = quality_pixels.sort_values([
                'avg_glacier_fraction',  'distance_to_aws'
            ], ascending=[False, True])
            
            # Select the 2 closest best performing pixels
            selected_pixels = quality_pixels.head(2)
            selected_pixel_ids = set(selected_pixels['pixel_id'])
            
            self.logger.info(f"Selected 2 closest best performing pixels from {len(modis_data['pixel_id'].unique())} total pixels")
            self.logger.info(f"AWS station: {aws_station['name']} at ({aws_lat:.4f}, {aws_lon:.4f})")
            
            for _, pixel in selected_pixels.iterrows():
                self.logger.info(f"  Pixel {pixel['pixel_id']}: glacier_fraction={pixel['avg_glacier_fraction']:.3f}, "
                               f"distance={pixel['distance_to_aws']:.2f}km, observations={pixel['n_observations']}")
            
            # Filter MODIS data
            filtered_data = modis_data[modis_data['pixel_id'].isin(selected_pixel_ids)].copy()
            self.logger.info(f"Filtered MODIS data from {len(modis_data)} to {len(filtered_data)} observations")
            
            return filtered_data
            
        except Exception as e:
            self.logger.error(f"Error applying pixel selection for {glacier_id}: {e}")
            self.logger.warning("Falling back to using all pixels")
            return modis_data
    
    def _apply_standard_pixel_selection(self, modis_data: pd.DataFrame, glacier_id: str) -> pd.DataFrame:
        """Fallback pixel selection when AWS coordinates unavailable."""
        try:
            pixel_summary = modis_data.groupby('pixel_id').agg({
                'glacier_fraction': 'mean',
                'albedo': 'count',
                'ndsi': 'mean'
            }).reset_index()
            
            pixel_summary.columns = ['pixel_id', 'avg_glacier_fraction', 'n_observations', 'avg_ndsi']
            
            quality_pixels = pixel_summary[
                (pixel_summary['avg_glacier_fraction'] > 0.1) & 
                (pixel_summary['n_observations'] > 10)
            ].copy()
            
            if len(quality_pixels) == 0:
                return modis_data
            
            selected_pixels = quality_pixels.nlargest(2, 'avg_glacier_fraction')
            selected_pixel_ids = set(selected_pixels['pixel_id'])
            
            return modis_data[modis_data['pixel_id'].isin(selected_pixel_ids)].copy()
            
        except Exception as e:
            self.logger.error(f"Error in standard pixel selection: {e}")
            return modis_data
    
    def _perform_enhanced_statistical_analysis(self, merged_data: pd.DataFrame) -> Dict[str, Any]:
        """Perform enhanced statistical analysis (from pivot_based_main.py)."""
        statistics = {}
        
        # Get available MODIS methods
        modis_methods = [col for col in merged_data.columns if col in ['MCD43A3', 'MOD09GA', 'MOD10A1']]
        
        if not modis_methods:
            self.logger.warning("No MODIS methods found in merged data")
            return statistics
        
        self.logger.info(f"Analyzing methods: {modis_methods}")
        
        # Calculate comprehensive metrics for each method
        statistics['method_comparison'] = {}
        
        for method in modis_methods:
            if method in merged_data.columns and 'AWS' in merged_data.columns:
                valid_data = merged_data[[method, 'AWS']].dropna()
                
                if len(valid_data) > 0:
                    aws_vals = valid_data['AWS']
                    modis_vals = valid_data[method]
                    
                    # Comprehensive statistics
                    r, p = scipy_stats.pearsonr(modis_vals, aws_vals)
                    rmse = np.sqrt(np.mean((modis_vals - aws_vals)**2))
                    mae = np.mean(np.abs(modis_vals - aws_vals))
                    bias = np.mean(modis_vals - aws_vals)
                    
                    # Additional enhanced metrics
                    diff = modis_vals - aws_vals
                    abs_diff = np.abs(diff)
                    rel_diff = diff / aws_vals * 100
                    
                    # Regression analysis
                    slope, intercept, r_value, p_value, std_err = scipy_stats.linregress(modis_vals, aws_vals)
                    
                    metrics = {
                        'n_samples': len(valid_data),
                        'r': r,
                        'p': p,
                        'r_squared': r**2,
                        'rmse': rmse,
                        'mae': mae,
                        'bias': bias,
                        'med_abs_error': np.median(abs_diff),
                        'std_error': np.std(diff),
                        'mean_rel_bias': np.mean(rel_diff),
                        'mean_abs_rel_error': np.mean(np.abs(rel_diff)),
                        'slope': slope,
                        'intercept': intercept,
                        'slope_stderr': std_err,
                        'mean_obs': np.mean(aws_vals),
                        'mean_pred': np.mean(modis_vals),
                        'std_obs': np.std(aws_vals),
                        'std_pred': np.std(modis_vals),
                        'within_5pct': np.sum(np.abs(rel_diff) <= 5) / len(valid_data) * 100,
                        'within_10pct': np.sum(np.abs(rel_diff) <= 10) / len(valid_data) * 100,
                        'within_15pct': np.sum(np.abs(rel_diff) <= 15) / len(valid_data) * 100
                    }
                    
                    statistics['method_comparison'][method] = metrics
                    
                    self.logger.info(f"{method}: n={metrics['n_samples']}, r={metrics['r']:.4f}, "
                                   f"RMSE={metrics['rmse']:.4f}, Bias={metrics['bias']:.4f}")
        
        # Create method ranking
        if len(statistics['method_comparison']) > 1:
            ranking_data = []
            for method, metrics in statistics['method_comparison'].items():
                ranking_data.append({
                    'method': method,
                    'r_squared': metrics['r_squared'],
                    'rmse': metrics['rmse'],
                    'bias': abs(metrics['bias']),
                    'correlation': metrics['r'],
                    'n_samples': metrics['n_samples']
                })
            
            ranking_df = pd.DataFrame(ranking_data)
            ranking_df['r2_rank'] = ranking_df['r_squared'].rank(ascending=False)
            ranking_df['rmse_rank'] = ranking_df['rmse'].rank()
            ranking_df['bias_rank'] = ranking_df['bias'].rank() 
            ranking_df['overall_rank'] = (ranking_df['r2_rank'] + ranking_df['rmse_rank'] + ranking_df['bias_rank']) / 3
            
            statistics['ranking'] = ranking_df.sort_values('overall_rank')
        
        return statistics
    
    def _perform_outlier_analysis(self, merged_data: pd.DataFrame) -> Dict[str, Any]:
        """Perform residual-based outlier analysis (from pivot_based_main.py)."""
        outlier_results = {}
        
        modis_methods = [col for col in merged_data.columns if col in ['MCD43A3', 'MOD09GA', 'MOD10A1']]
        
        if not modis_methods or 'AWS' not in merged_data.columns:
            return outlier_results
        
        self.logger.info("Performing residual-based outlier detection (2.5 sigma threshold)")
        
        stats_with_outliers = {}
        stats_without_outliers = {}
        outlier_info = {}
        
        for method in modis_methods:
            mask = merged_data[[method, 'AWS']].notna().all(axis=1)
            if mask.sum() == 0:
                continue
                
            x_all = merged_data.loc[mask, 'AWS']
            y_all = merged_data.loc[mask, method]
            
            # Stats with outliers
            r_all, _ = scipy_stats.pearsonr(x_all, y_all)
            rmse_all = np.sqrt(np.mean((y_all - x_all)**2))
            mae_all = np.mean(np.abs(y_all - x_all))
            bias_all = np.mean(y_all - x_all)
            stats_with_outliers[method] = {
                'n': len(x_all), 'r': r_all, 'rmse': rmse_all, 
                'mae': mae_all, 'bias': bias_all
            }
            
            # Remove residual outliers
            slope, intercept = np.polyfit(x_all, y_all, 1)
            predicted = slope * x_all + intercept
            residuals = y_all - predicted
            residual_threshold = 2.5 * residuals.std()
            residual_outliers = np.abs(residuals) > residual_threshold
            
            outlier_series = pd.Series(residual_outliers, index=mask[mask].index).reindex(merged_data.index, fill_value=False)
            clean_mask = mask & ~outlier_series
            
            n_outliers = residual_outliers.sum()
            outlier_info[method] = {
                'n_total': len(x_all),
                'n_outliers': n_outliers,
                'outlier_percentage': (n_outliers / len(x_all)) * 100,
                'residual_threshold': residual_threshold,
                'outlier_indices': mask[mask].index[residual_outliers].tolist()
            }
            
            if clean_mask.sum() > 0:
                x_clean = merged_data.loc[clean_mask, 'AWS']
                y_clean = merged_data.loc[clean_mask, method]
                
                r_clean, _ = scipy_stats.pearsonr(x_clean, y_clean)
                rmse_clean = np.sqrt(np.mean((y_clean - x_clean)**2))
                mae_clean = np.mean(np.abs(y_clean - x_clean))
                bias_clean = np.mean(y_clean - x_clean)
                stats_without_outliers[method] = {
                    'n': len(x_clean), 'r': r_clean, 'rmse': rmse_clean,
                    'mae': mae_clean, 'bias': bias_clean
                }
                
                r_improvement = ((r_clean - r_all) / abs(r_all)) * 100 if r_all != 0 else 0
                rmse_improvement = ((rmse_all - rmse_clean) / rmse_all) * 100 if rmse_all != 0 else 0
                
                outlier_info[method].update({
                    'r_improvement_pct': r_improvement,
                    'rmse_improvement_pct': rmse_improvement
                })
                
                self.logger.info(f"{method}: Removed {n_outliers} outliers ({n_outliers/len(x_all)*100:.1f}%), "
                               f"r improved by {r_improvement:.1f}%")
        
        return {
            'stats_with_outliers': stats_with_outliers,
            'stats_without_outliers': stats_without_outliers,
            'outlier_info': outlier_info
        }
    
    def _generate_enhanced_plots(self, merged_data: pd.DataFrame, statistics: Dict[str, Any], 
                               glacier_id: str, output_dir: str):
        """Generate enhanced plots - restore original pivot_based_main.py functionality."""
        # This should generate ALL the original plots like it used to
        try:
            # Call the method that generates all original plots
            self._generate_all_original_plots(merged_data, statistics, glacier_id, output_dir)
        except Exception as e:
            self.logger.error(f"Error generating enhanced plots: {e}")
            # Fallback to user style plots
            try:
                self._generate_user_style_plots(merged_data, statistics, glacier_id, output_dir)
            except Exception as fallback_error:
                self.logger.error(f"Fallback plotting also failed: {fallback_error}")
    
    def _generate_comprehensive_plots_with_fallback(self, merged_data: pd.DataFrame, statistics: Dict[str, Any], 
                                                  glacier_id: str, output_dir: str, plot_mode: str):
        """Generate comprehensive plots based on plot_mode with fallback to original system."""
        # ALWAYS use original plotting system until refined system is fully working
        self.logger.info(f"Using original 7-plot system (requested mode: {plot_mode})...")
        try:
            self._generate_user_style_plots(merged_data, statistics, glacier_id, output_dir)
            self.logger.info("Successfully generated all plots using original system")
        except Exception as e:
            self.logger.error(f"Error in original plotting system: {e}")
            raise

    def _generate_comprehensive_plots(self, merged_data: pd.DataFrame, statistics: Dict[str, Any], 
                                    glacier_id: str, output_dir: str, plot_mode: str):
        """Generate refined visualization suite with redundancy elimination and dashboard options."""
        plots_dir = os.path.join(output_dir, 'plots')
        
        modis_methods = [col for col in merged_data.columns if col in ['MCD43A3', 'MOD09GA', 'MOD10A1']]
        
        if not modis_methods or 'AWS' not in merged_data.columns:
            self.logger.warning("Insufficient data for comprehensive plotting")
            return
        
        # Get configuration
        plot_config = self.config.get('visualization', {}).get('plot_output', {})
        
        # Prepare data for plotting
        aws_data = merged_data['AWS'].dropna()
        modis_data = {}
        for method in modis_methods:
            modis_data[method] = merged_data[method].dropna()
        
        glacier_name = glacier_id.title()
        
        # Generate plots based on mode
        if plot_mode in ['individual', 'both']:
            # Generate individual specialized plots (eliminating redundancy)
            self.logger.info("Generating refined individual plots...")
            self._generate_refined_individual_plots(aws_data, modis_data, statistics, 
                                                  glacier_name, glacier_id, plots_dir)
        
        if plot_mode in ['dashboard', 'both']:
            # Generate comprehensive dashboard
            self.logger.info("Generating comprehensive dashboard...")
            self._generate_dashboard_plot(aws_data, modis_data, statistics, 
                                        glacier_name, glacier_id, plots_dir)
        
        # Generate spatial maps if configured
        self._create_spatial_maps(glacier_id, output_dir)
    
    def _generate_refined_individual_plots(self, aws_data: pd.Series, modis_data: Dict[str, pd.Series], 
                                         statistics: Dict[str, Any], glacier_name: str, 
                                         glacier_id: str, plots_dir: str):
        """Generate individual specialized plots with redundancy elimination."""
        plot_config = self.config.get('visualization', {}).get('plot_output', {})
        
        # 1. Outlier Analysis (keep exact style - always first to set baseline)
        if plot_config.get('include_outlier_analysis', True):
            # This would call the existing outlier analysis plot - keeping exact current style
            self.logger.info("Generating outlier analysis (keeping exact current style)")
            # Note: Implementation would preserve the exact current outlier analysis plot
        
        # 2. Seasonal Analysis (keep exact style - register box plots)
        if plot_config.get('include_seasonal_analysis', True):
            # This would call the existing seasonal analysis plot
            self.logger.info("Generating seasonal analysis (keeping exact current style)")
            self.plot_generator._generated_boxplots.add("seasonal_monthly_boxplots")
            # Note: Implementation would preserve the exact current seasonal analysis plot
        
        # 3. Distribution Analysis (refined - histograms only, no duplicate box plots)
        if plot_config.get('include_distribution_analysis', True):
            output_path = os.path.join(plots_dir, f"{glacier_id}_distribution_analysis.png")
            fig = self.plot_generator.create_refined_distribution_analysis(
                modis_data, glacier_name, output_path
            )
            if fig:
                plt.close(fig)
        
        # 4. Method Comparison (refined - scatter plots only, no duplicate box plots)  
        if plot_config.get('include_method_comparison', True):
            output_path = os.path.join(plots_dir, f"{glacier_id}_method_comparison.png")
            fig = self.plot_generator.create_refined_method_comparison(
                aws_data, modis_data, glacier_name, output_path
            )
            if fig:
                plt.close(fig)
        
        # 5. Temporal Overview (refined - time series only, no duplicate box plots)
        if plot_config.get('include_temporal_overview', True):
            output_path = os.path.join(plots_dir, f"{glacier_id}_temporal_overview.png")
            # Create time series plot (avoiding any box plots that duplicate seasonal analysis)
            if hasattr(self.plot_generator, 'create_time_series_plot'):
                combined_data = pd.DataFrame(modis_data)
                combined_data['AWS'] = aws_data
                combined_data = combined_data.reset_index()
                combined_data['date'] = combined_data.index
                
                long_data = []
                for method in modis_data.keys():
                    method_df = combined_data[['date', method]].dropna()
                    method_df['method'] = method
                    method_df['albedo'] = method_df[method]
                    long_data.append(method_df[['date', 'method', 'albedo']])
                
                if long_data:
                    temporal_data = pd.concat(long_data, ignore_index=True)
                    fig = self.plot_generator.create_time_series_plot(
                        temporal_data, title=f"{glacier_name} - Temporal Analysis", output_path=output_path
                    )
                    if fig:
                        plt.close(fig)
        
        # 6. Statistical Summary (refined - tables/metrics only)
        if plot_config.get('include_statistical_summary', True):
            output_path = os.path.join(plots_dir, f"{glacier_id}_statistical_summary.png")
            # Create statistical summary without redundant visualizations
            self._create_statistical_summary_plot(statistics, glacier_name, output_path)
    
    def _generate_dashboard_plot(self, aws_data: pd.Series, modis_data: Dict[str, pd.Series], 
                               statistics: Dict[str, Any], glacier_name: str, 
                               glacier_id: str, plots_dir: str):
        """Generate comprehensive dashboard plot."""
        output_path = os.path.join(plots_dir, f"{glacier_id}_comprehensive_dashboard.png")
        
        fig = self.plot_generator.create_comprehensive_dashboard(
            aws_data, modis_data, statistics, glacier_name, output_path
        )
        
        if fig:
            plt.close(fig)
            self.logger.info(f"Generated comprehensive dashboard: {output_path}")
    
    def _create_statistical_summary_plot(self, statistics: Dict[str, Any], glacier_name: str, output_path: str):
        """Create statistical summary plot with tables and metrics."""
        if 'method_comparison' not in statistics:
            return
        
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.axis('off')
        
        # Create comprehensive statistics table
        table_data = []
        headers = ['Method', 'N', 'R', 'R', 'RMSE', 'Bias', 'MAE', 'Slope', 'Intercept']
        
        for method, metrics in statistics['method_comparison'].items():
            table_data.append([
                method,
                f"{metrics.get('n_samples', 0)}",
                f"{metrics.get('r', 0):.4f}",
                f"{metrics.get('r_squared', 0):.4f}",
                f"{metrics.get('rmse', 0):.4f}",
                f"{metrics.get('bias', 0):.4f}",
                f"{metrics.get('mae', 0):.4f}",
                f"{metrics.get('slope', 0):.4f}",
                f"{metrics.get('intercept', 0):.4f}"
            ])
        
        if table_data:
            table = ax.table(cellText=table_data,
                           colLabels=headers,
                           cellLoc='center',
                           loc='center')
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1.2, 2)
            
            # Style the table
            for i in range(len(headers)):
                table[(0, i)].set_facecolor('#4CAF50')
                table[(0, i)].set_text_props(weight='bold', color='white')
        
        ax.set_title(f'{glacier_name} - Statistical Summary', 
                    fontsize=16, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close(fig)
        
        self.logger.info(f"Generated statistical summary: {output_path}")
    
    def _generate_user_style_plots(self, merged_data: pd.DataFrame, statistics: Dict[str, Any], 
                                  glacier_id: str, output_dir: str):
        """Generate user-style plots (from pivot_based_main.py)."""
        plots_dir = os.path.join(output_dir, 'plots')
        
        modis_methods = [col for col in merged_data.columns if col in ['MCD43A3', 'MOD09GA', 'MOD10A1']]
        
        if not modis_methods or 'AWS' not in merged_data.columns:
            self.logger.warning("Insufficient data for plotting")
            return
        
        # Extract month data for seasonal coloring
        months = merged_data.index.month
        unique_months = sorted(months.unique())
        month_names = {6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct'}
        colors = plt.cm.Set1(np.linspace(0, 1, len(unique_months)))
        
        method_colors = {'MCD43A3': 'darkblue', 'MOD09GA': 'darkred', 'MOD10A1': 'darkgreen'}
        
        # Create comprehensive scatter plot analysis
        fig, axes = plt.subplots(1, len(modis_methods), figsize=(6*len(modis_methods), 6))
        if len(modis_methods) == 1:
            axes = [axes]
        
        fig.suptitle(f'{glacier_id.upper()} Glacier - MODIS vs AWS Albedo Comparison\n(Enhanced Analysis)', 
                     fontsize=16, fontweight='bold')
        
        for i, method in enumerate(modis_methods):
            ax = axes[i]
            
            valid_data = merged_data[[method, 'AWS']].dropna()
            
            if len(valid_data) == 0:
                ax.text(0.5, 0.5, f'No valid data\nfor {method}', 
                       ha='center', va='center', transform=ax.transAxes)
                continue
            
            # Plot data points colored by season
            for month, color in zip(unique_months, colors):
                if month in month_names:
                    month_mask = merged_data.index.month == month
                    month_data = valid_data[valid_data.index.isin(merged_data[month_mask].index)]
                    
                    if len(month_data) > 0:
                        month_label = month_names[month]
                        ax.scatter(month_data[method], month_data['AWS'], 
                                 c=[color], alpha=0.6, s=30, 
                                 label=month_label if i == 0 else "")
            
            # Add regression line
            if method in statistics.get('method_comparison', {}):
                stats_data = statistics['method_comparison'][method]
                slope = stats_data['slope']
                intercept = stats_data['intercept']
                
                x_min, x_max = valid_data[method].min(), valid_data[method].max()
                x_range = np.linspace(x_min, x_max, 50)
                y_pred = slope * x_range + intercept
                ax.plot(x_range, y_pred, color=method_colors.get(method, 'black'), 
                       linewidth=2, alpha=0.8, 
                       label=f'Regression (y={slope:.3f}x+{intercept:.3f})')
            
            # Add 1:1 line
            ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, linewidth=1, label='1:1 line')
            
            ax.set_xlim(0, 1)
            ax.set_ylim(0, 1)
            ax.set_xlabel('Satellite Albedo')
            ax.set_ylabel('AWS Albedo')
            ax.set_title(f'{method} vs AWS')
            ax.grid(True, alpha=0.3)
            
            # Add statistics text box
            if method in statistics.get('method_comparison', {}):
                stats_data = statistics['method_comparison'][method]
                method_nonull_count = merged_data[method].count()
                stats_text = (f"{method}:\n"
                             f"n = {method_nonull_count}\n"
                             f"r = {stats_data['r']:.3f}\n"
                             f"RMSE = {stats_data['rmse']:.3f}\n"
                             f"MAE = {stats_data['mae']:.3f}\n"
                             f"Bias = {stats_data['bias']:.3f}")
                
                ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
                        verticalalignment='top', horizontalalignment='left',
                        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8),
                        fontsize=8, family='monospace')
            
            # Add legend only for the first plot
            if i == 0:
                seasonal_legend = [plt.Line2D([0], [0], marker='o', color='w', 
                                            markerfacecolor=color, markersize=6, 
                                            label=month_names.get(month, f'M{month}'))
                                for month, color in zip(unique_months, colors) 
                                if month in month_names]
                seasonal_legend.append(plt.Line2D([0], [0], color='k', linestyle='--', alpha=0.5, label='1:1 line'))
                ax.legend(handles=seasonal_legend, loc='lower right', fontsize=8)
        
        plt.tight_layout()
        
        # Save the plot
        plot_path = os.path.join(plots_dir, f"{glacier_id}_comprehensive_analysis.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"Created comprehensive analysis plot: {plot_path}")
    
    def _create_spatial_maps(self, glacier_id: str, output_dir: str):
        """Generate spatial maps (simplified version from pivot_based_main.py)."""
        try:
            maps_dir = os.path.join(output_dir, 'maps')
            os.makedirs(maps_dir, exist_ok=True)
            
            self.logger.info("Generating spatial maps...")
            
            # Simplified spatial mapping - would include full implementation from pivot_based_main.py
            if hasattr(self, '_last_modis_data_original'):
                self.logger.info("Creating pixel location maps...")
                # Spatial mapping code would go here
            
            self.logger.info(f"Generated spatial maps in: {maps_dir}")
            
        except Exception as e:
            self.logger.error(f"Error creating spatial maps: {e}")
    
    def _export_enhanced_results(self, statistics: Dict[str, Any], merged_data: pd.DataFrame,
                               glacier_id: str, output_dir: str, analysis_mode: str) -> Dict[str, Any]:
        """Export enhanced analysis results."""
        results_dir = os.path.join(output_dir, 'results')
        
        # Export method comparison
        if 'method_comparison' in statistics:
            comparison_df = pd.DataFrame(statistics['method_comparison']).T
            comparison_file = os.path.join(results_dir, f"{glacier_id}_{analysis_mode}_method_comparison.csv")
            comparison_df.to_csv(comparison_file)
            self.logger.info(f"Exported method comparison: {comparison_file}")
        
        # Export ranking if available
        if 'ranking' in statistics:
            ranking_file = os.path.join(results_dir, f"{glacier_id}_{analysis_mode}_method_ranking.csv")
            statistics['ranking'].to_csv(ranking_file, index=False)
            self.logger.info(f"Exported method ranking: {ranking_file}")
        
        # Export merged data
        merged_file = os.path.join(results_dir, f"{glacier_id}_{analysis_mode}_merged_data.csv")
        merged_data.to_csv(merged_file)
        self.logger.info(f"Exported merged data: {merged_file}")
        
        return {
            'glacier_id': glacier_id,
            'analysis_type': analysis_mode,
            'analysis_timestamp': get_timestamp(),
            'output_directory': output_dir,
            'n_merged_observations': len(merged_data),
            'available_methods': [col for col in merged_data.columns if col in ['MCD43A3', 'MOD09GA', 'MOD10A1']],
            'date_range': {
                'start': merged_data.index.min().strftime('%Y-%m-%d'),
                'end': merged_data.index.max().strftime('%Y-%m-%d')
            },
            'statistics': statistics
        }


def main():
    """Main function for command line usage."""
    parser = argparse.ArgumentParser(description='Unified MODIS Albedo Analysis Engine')
    parser.add_argument('--glacier', type=str, help='Glacier ID to process')
    parser.add_argument('--all-glaciers', action='store_true', 
                       help='Process all glaciers in configuration')
    parser.add_argument('--analysis-mode', type=str, default='auto',
                       choices=['auto', 'basic', 'enhanced', 'comprehensive'],
                       help='Analysis mode: auto (default), basic, enhanced, or comprehensive')
    parser.add_argument('--selected-pixels', action='store_true',
                       help='Use pixel selection algorithm for enhanced/comprehensive modes')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                       help='Path to configuration file')
    parser.add_argument('--output-summary', type=str, 
                       help='Output file for summary results')
    
    args = parser.parse_args()
    
    if not args.glacier and not args.all_glaciers:
        parser.error("Must specify either --glacier or --all-glaciers")
    
    try:
        # Initialize unified engine
        engine = AlbedoAnalysisEngine(args.config)
        
        # Process glaciers
        all_results = []
        
        if args.glacier:
            # Process single glacier
            result = engine.process_glacier(
                args.glacier, 
                analysis_mode=args.analysis_mode,
                use_selected_pixels=args.selected_pixels
            )
            all_results.append(result)
            
        elif args.all_glaciers:
            # Process all glaciers
            glacier_sites_config = load_config('config/glacier_sites.yaml')
            glacier_ids = list(glacier_sites_config['glaciers'].keys())
            
            for glacier_id in glacier_ids:
                try:
                    result = engine.process_glacier(
                        glacier_id,
                        analysis_mode=args.analysis_mode,
                        use_selected_pixels=args.selected_pixels
                    )
                    all_results.append(result)
                except Exception as e:
                    logging.error(f"Failed to process {glacier_id}: {e}")
                    continue
        
        # Export summary if requested
        if args.output_summary and all_results:
            summary_df = pd.DataFrame([
                {
                    'glacier_id': result['glacier_id'],
                    'analysis_type': result['analysis_type'],
                    'analysis_timestamp': result['analysis_timestamp'],
                    'n_merged_observations': result.get('n_merged_observations', 'N/A'),
                    'output_directory': result['output_directory']
                }
                for result in all_results
            ])
            summary_df.to_csv(args.output_summary, index=False)
            print(f"Summary exported to: {args.output_summary}")
        
        print(f"\nUnified analysis completed for {len(all_results)} glacier(s)")
        print(f"Analysis mode used: {args.analysis_mode}")
        print(f"Pixel selection: {'enabled' if args.selected_pixels else 'disabled'}")
        
        # Print key results
        for result in all_results:
            print(f"\n{result['glacier_id']}:")
            print(f"  - Analysis type: {result['analysis_type']}")
            if 'n_merged_observations' in result:
                print(f"  - {result['n_merged_observations']} merged observations")
            if 'available_methods' in result:
                print(f"  - Methods: {', '.join(result['available_methods'])}")
            if 'date_range' in result:
                print(f"  - Date range: {result['date_range']['start']} to {result['date_range']['end']}")
            print(f"  - Output: {result['output_directory']}")
        
    except Exception as e:
        logging.error(f"Analysis engine failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()